\documentclass[10pt]{article}
\textwidth=7in
\textheight=9.5in
\topmargin=-1in
\headheight=0in
\headsep=.5in
\hoffset=  -.75in
\oddsidemargin= 0.5in
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsxtra}     % Use various AMS packages
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{comment}
\usepackage{marginnote}
\usepackage{xurl}
\usepackage{xcolor}



\pagestyle{empty}

\newcommand{\sol}{\noindent \textbf{Solution: }}
\newcommand{\com}{\noindent \textbf{Comments: }} 
\newcommand{\example}{\noindent \textbf{Example: }}
\newcommand{\theorem}{\noindent \textbf{Theorem: }}
\renewcommand*{\iint}{\displaystyle\int}
\newcommand{\uu}{{\textbf{u}}}
\newcommand{\vv}{{\textbf{v}}}
\newcommand{\ii}{{\textbf{i}}}
\newcommand{\jj}{{\textbf{j}}}
\newcommand{\kk}{{\textbf{k}}}
\newcommand{\ww}{{\textbf{w}}}
\newcommand{\pd}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\grad}{\nabla}
\newcommand{\fhat}{\hat{f}}
\newcommand{\EE}{{\mathbb{E}}}
\newcommand{\MM}{{\mathcal{M}}}
\newcommand{\Var}{{\rm Var}}
\newcommand{\Bias}{{\rm Bias}}
\newcommand{\Cor}{{\rm Cor}}
\newcommand{\Cov}{{\rm Cov}}
\newcommand{\impbox}[1] {	
	\begin{center}\fbox{\parbox{.95\linewidth}{ #1 }}\end{center} 
	
	\vspace{.1in} 
}
\newcommand{\impstar}{\marginnote{\Huge{$*$}}}
\newcommand{\eps}{\epsilon} 
\newcommand{\SE}{{\rm SE}}
\renewcommand{\th}{\theta}


\begin{document}

{\Large{Introduction to Statistical Learning -- Notes }} \hspace{1.7in} 

\setlength{\unitlength}{1in}

\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

\vspace{.10in}

\renewcommand*{\theenumi}{\thesubsubsection.\arabic{enumi}}
\renewcommand*{\theenumii}{\theenumi.\arabic{enumii}}


\section{Introduction} 

\begin{enumerate}
	\item \textbf{Statistical learning} refers to a vast set of tools for understanding data.
	\item \textbf{Supervised learning} builds a statistical model for predicting or estimating an output based on one or more inputs. Examples include regression, classification, neural networks. Think of it as modeling a function from data.
	\item \textbf{Unsupervised learning} has inputs, but no supervising output (no labeled training data).  These tools look for relationships and patterns within input data. Examples: clustering, principal component analysis.
	\item \textbf{Continuous or quantitative} refers to numerical values. Modeling continuous data is referred to as \textbf{regression}.
	\item \textbf{Categorical or qualitative} refers to non-numerical data. Modeling categorical data is referred to as \textbf{classification}.
	\item Grouping data based upon observed characteristics is called \textbf{clustering}.  This is an example of unsupervised learning.
\end{enumerate}

\section{Statistical Learning} 

\subsection{What is Statistical Learning?}

\begin{enumerate}	
	\item \textbf{Input variables} also known as \textbf{predictors}, \textbf{independent variables}, \textbf{features}, or sometimes just \textbf{variables}, are the features which we typically know, and use to make predictions about an output variable. Input variables are typically denoted by $X$, possibly with a subscript.
	\item The \textbf{output variable}, also sometimes known as the \textbf{response} or \textbf{dependent variable} is the quantity we are trying to model and is typically denoted $Y$.
	\item Suppose we observe a quantitative response $Y$ and $p$ different predictors $X_1, X_2, ..., X_p$.  We assume there is a relationship between $Y$ and $X = (X_1, ... , X_p)$ of the form:
	$$Y = f(X) + \epsilon$$
	where $f$ is a fixed but unknown function and $\epsilon$ is an \textbf{error term}, independent of $X$ with mean 0.
	
	We say that $f$ represents the \textbf{systematic information} that $X$ provides about $Y$.
	
	Statistical learning refers to a set of approaches to estimating $f$.
\end{enumerate}

\vspace{.2in} 

\subsubsection{Why Estimate $f$?}

\begin{enumerate}
	
	\item We want to estimate $f$ for two main reasons: \textbf{prediction} and \textbf{inference}.
	
	\item Using the above formulation, since the error term $\epsilon$ averages to 0, we can predict $Y$ using:
	$$\hat{Y} = \hat{f}(X)$$
	where $\hat{f}$ is the estimate for $f$ and $\hat{Y}$ is our subsequent prediction for $Y$.
	\item $\hat{f}$ is treated like a \textbf{black box}, in the sense that we don't particularly care about the details of $\hat{f}$ as long as it does a good job of predicting $Y$.
	\item The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on \textbf{reducible error} and \textbf{irreducible error}.  \textbf{Reducible error} is error that can potentially be reduced by improving the estimate $\hat{f}$. 
	\item Even if reducible error were reduced completely, so that we have a perfect estimate of $f$, there would still be \textbf{irreducible error} since $Y$ is presumed to depend on $\epsilon$ which cannot possibly be predicted using $X$.  This is because $\epsilon$ may contain unmeasured variables which affect $Y$.  Or it may be that $\epsilon$ contained unmeasurable variation.
	
	\item We can see reducible and irreducible error numerically.  Fix $\fhat$ and predictors $X$ so that the only variability comes in $Y$ from $\epsilon$.  Then:
	\begin{align*}
	\EE(Y - \hat{Y}) ^2 &= \EE[f(X) + \epsilon - \fhat(X) ]^2 \\
	&= \EE[ (f(X) - \fhat(X)) + \epsilon]^2 \\
	&= \EE[(f(X) - \fhat(X))^2 + 2\epsilon (f(X) - \fhat(X)) + \epsilon^2] \\
	&= \EE(f(X) - \fhat(X))^2 + \EE(2\epsilon (f(X) - \fhat(X))) + \EE(\epsilon^2) \tag{linearity of expectation} \\
	&= [f(X) - \fhat(X)]^2 + 2(f(X) - \fhat(X)) \EE(\epsilon) + {\rm{Var}}(\epsilon) \tag{$X$ is fixed}\\
	&= \underbrace{[f(X) - \fhat(X)]^2}_\text{Reducible}  + \underbrace{{\rm{Var}}(\epsilon)}_\text{Irreducible} \tag{$\EE(\epsilon) = 0$}  
	\end{align*}
	\item Irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$.  This bound is almost always unknown in practice.
	
	\item Often, the goal is to understand the relationship between $Y$ and $X_1, ... X_p$. And so we try to estimate $f$.  But now, when prediction isn't the goal, we do not treat $\fhat$ as a black box.  
	
	\item In inference, we might be interested in questions like:
	\begin{itemize}
		\item Which predictors are associated with the response?
		\item What is the relationship between the response and the predictor?
		\item Can the relationship between $Y$ and each predictor be summarized using a linear equation, or is the relationship more complicated?
	\end{itemize}
	
	\item Sometimes the goal can be a combination of prediction and inference.  The type of model used might be determined by the nature of the goal.  Linear modelling is relatively easy to interpret, so lends itself well to inference, whereas more complicated models may do a better job at prediction, but be more difficult to interpret.
\end{enumerate} 

\vspace{.2in} 

\subsubsection{How Do We Estimate $f$?} 

\begin{enumerate}
	\item \textbf{Notation:} $n$ is the number of data points in our \textbf{training data} -- the observations we use to train or teach our estimate $\fhat$.  Let $x_{ij}$ be the value of the $j$th predictor in the $i$th observation.  And let $y_i$ be the value of the response variable in the $i$th observation.  Therefore, the training data is:
	$$\{ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \} \hspace{.2in} \text{where} \hspace{.2in} x_i = (x_{i1}, x_{i2}, \dots, x_{ip})^T$$
	\item Broadly speaking, statistical learning methods fall into one of two types: \textbf{parametric} and \textbf{non-parametric}.  
	\item {\color{brown} A rough characterization of the difference might be that parametric models are ``form then data" whereas non-parametric models are ``data then form.'' }
	\item \textbf{Parametric models} involve a two-step approach:
		\begin{itemize}
			\item First, make an assumption about the form of $f$.  One simple example is that $f$ is linear in $X$:
			$$f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p$$
			This is a \textbf{linear model}.  With the assumption that $f$ is linear, the problem of estimating $f$ becomes the problem of estimating the \textbf{parameters} $\beta_0, \dots, \beta_p$.
			\item After making the assumption about the model (model selection), \textbf{train} or \textbf{fit} the model.  One approach to fitting the model is called \textbf{(ordinary) least squares}, but there are many.
		\end{itemize}
	\item Examples of parametric methods: regression.
	\item The disadvantage of a parametric approach is that $\fhat$ will usually not match the true, unknown $f$. 
	\item We can try to address this problem by choosing \textbf{flexible} models that can fit many $f$ well.  In general, a more flexible model involves a greater number of parameters.  This can lead to \textbf{overfitting} the data which means that the \textbf{noise} in the data is followed too closely.
	\item \textbf{Non-parametric methods} do not make explicit assumptions about the functional form of $f$.  Instead, they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly.
	\item The advantage of non-parametric models is that they have the potential to accurately fit a wider range of possible shapes for $f$. 
	\item The disadvantage is that whereas parametric methods reduce the problem of estimating $f$ to estimating a few parameters, estimating $f$ using non-parametric methods do not admit this simplification and, consequently, requires a large number of observations to obtain an accurate $\fhat$.
	\item Examples of non-parametric methods:
	\begin{itemize}
		\item \textbf{Kernel methods} like kernel regression or support vector machines.  
		\item \textbf{Splines}
		\item \textbf{Decision Trees}
		\item \textbf{Nearest neighbor methods} like KNN.
	\end{itemize}	
	\item We can judge these models by their \textbf{smoothness}.  {\color{brown} This is a somewhat under-defined quality of a model -- it speaks to the dichotomy between ``smooth'' and ``bumpy''.}  Lower levels of smoothness may be indicative of overfitting.  Choosing the correct amount of smoothness will be discussed later.

\end{enumerate}

\vspace{.2in} 
	
\subsubsection{The Trade-Off Between Prediction Accuracy and Model Interpretability} 

\begin{enumerate}
	\item \impstar As flexibility of a model increases, interpretability generally goes down.
	\item In order, from least flexible to most flexible:
		\begin{itemize}
			\item Subset Selection Lasso
			\item Least Squares, linear regression
			\item Generalized Additive Models (GAMs), Trees
			\item Bagging, Boosting
			\item Support Vector Machines
			\item Deep Learning
		\end{itemize}
	\item If the goal is interpretability, a less flexible model may be preferable.  Whereas if prediction is the goal, it turns out that sometimes less flexible models make better predictions -- this is because of overfitting.
\end{enumerate}

\subsubsection{Supervised vs Unsupervised Learning} 	

\begin{enumerate}
	\item In \textbf{supervised learning}, the value of the response variable $Y$ is known.  In \textbf{unsupervised learning}, the vector of measurements of $X_1, ..., X_p$ may be known for our observations, but there is no associated response.
	\item Linear regression is supervised learning.
	\item \textbf{Cluster analysis}, a method to ascertain on the basis of $x_1, ..., x_n$ whether the observations in the data fall into relatively distinct groups.
	\item \textbf{Semi-supervised learning} is when the value of the response variable is available for only a strict subset of the observations -- this may be because collecting the data on the response variable is difficult or expensive.  This topic is not covered in this book. 
\end{enumerate}	

\subsubsection{Regression versus Classification Problems} 
\begin{enumerate}
	\item Variables can be either \textbf{quantitative} or \textbf{qualitative} (also known as \textbf{categorical}).
	\item Quantitative variables take on numerical values, whereas qualitative variables take on values in one of $K$ different \textbf{classes}, or categories.
	\item Problems with a quantitative response are referred to as \textbf{regression problems}, whereas those involving a qualitative response are \textbf{classification problems}. 
	\item Sometimes, the distinction is not always crisp.  Logistic regression is typically used with a two-class or binary response variable and thus is a classification problem.  However, since it estimates probabilities of being in a class, it can also be thought of as a regression problem.
	\item Whether predictors, the $X$'s are qualitative or quantitative is generally considered to be less important.  ``Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis."
\end{enumerate}

\vspace{.2in} 

\subsection{Assessing Model Accuracy} 
\begin{enumerate}
	\item It is necessary to introduce many different statistical learning approaches, because no one method dominates all others over all data sets.
	\item Selecting the best approach can be one of the most challenging parts of statistical learning in practice.
\end{enumerate}

\vspace{.2in} 

\subsubsection{Measuring the Quality of Fit} 
\begin{enumerate}
	\item To quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation, in the regression setting, the most commonly-used measure is the \textbf{mean squared error} (MSE):
	$$MSE = \dfrac{1}{n} \sum_{i = 1}^n (y_i - \fhat(x_i))^2 $$
	\item Because this MSE is computed using training data, it is also\ sometimes called \textbf{training MSE}.
	\item If prediction is our task, then predicting training data well is not really what we care about.Rather, we are interested in the accuracy of predictions when we apply our method to previously unseen \textbf{test data}.
	\item We want to minimize \textbf{test MSE}:
	$${\rm Ave}(y_0 - \fhat(x_0))^2$$
	over test observations $(x_0, y_0)$.
	\item How to select a method which minimizes test MSE?  We can reserve a set of observations that were not used to train the statistical learning method called the \textbf{test data}. 
	\item Or we might just take the method which minimizes training MSE, believing that this should also minimize the test MSE.  The problem here is that we can minimize training MSE by computing a spline which perfectly interpolates all data points, but which massively overfits the data, and so performs poorly on test MSE.
	\item Training MSE decreases monotonically with flexibility (greater degrees of freedom).  Test MSE looks like a U-shape.  One important method which will be discussed in the future for finding the lowest point of the U-curve is called \textbf{cross-validation}. 
	\item The irreducible error ${\rm Var}(\epsilon)$ is the lowest achievable test MSE.
\end{enumerate}

\vspace{.2in} 
\subsubsection{The Bias-Variance Trade-Off} 
\begin{enumerate}
	\item For a fixed observation $x_0$, the \textbf{expected test MSE at $x_0$} is the average value of $(y_0 - \fhat(x_0))^2$, where we average over the different possible realizations of training data that generated $\fhat$.  {\color{brown} Consider a particular training data set $\mathcal{T}$ coming from the set of all possible samples $\mathcal{S}$, presumably of the same size.  Then $\fhat$, really should be denoted $\fhat_{\mathcal{T}}$ since $\fhat$ is obtained by training on $\mathcal{T}$. Then the bias is:
	$$\text{Expected Test MSE at $x_0$} = \EE_{\mathcal{T} \in \mathcal{S}} ( y_0 - \fhat_{\mathcal{T}}(x_0) )^2 $$}
	\item \textbf{Bias} of a statistical learning method is the difference of the average value of prediction over different realizations of training data and the actual value:
	$$\text{Bias}(\fhat(x_0)) = \EE[\fhat(x_0) - f(x_0) ]  $$
	Generally speaking, perhaps as a rule of thumb, as flexibility increases, bias decreases.
	\item \textbf{Variance} of a statistical learning method is the variance in $\fhat(x_0)$ over different realizations of the training set.  
	$$\text{Variance}(\fhat(x_0)) = \EE\Big[ (\fhat(x_0) - \EE[\fhat(x_0)])^2  \Big]$$
	High variance means that small changes in the training data can result in large changes in $\fhat$.  Generally, more flexible statistical methods have higher variance.
	\item One can show that the expected test MSE at $x_0$ can be decomposed into the sum of the variance, the squared bias of, and the variance of the error:
	
	$$\EE[ ( y_0 - \fhat(x_0))^2] = \Var(\fhat(x_0)) + [\Bias(\fhat(x_0))]^2 + \Var(\epsilon) $$
	
	\noindent \textbf{Proof:} 
	\begin{align*}
	\EE[ ( y_0 - \fhat(x_0))^2] &= \EE[ ((f(x_0) + \epsilon ) - \fhat(x_0) )^2 ] \\
	&= \EE[ (f(x_0) - \fhat(x_0) + \eps)^2] \\
	&= \EE[ (f(x_0) - \fhat(x_0))^2 - 2\eps(f(x_0) - \fhat(x_0)) + \eps^2 ] \\
	&= \EE[ (f(x_0) - \fhat(x_0))^2 ]  - 2\EE[\eps(f(x_0) - \fhat(x_0)) ]  + \EE[\eps^2 ] \tag{linearity of expectation} \\
	&= \EE[ (f(x_0) - \fhat(x_0))^2 ]  - 2\EE[\eps]\EE[(f(x_0) - \fhat(x_0)) ]  + \EE[\eps^2 ] \tag{independence of $\eps$ and $\fhat(x_0)$} \\
	&= \EE[ (f(x_0) - \fhat(x_0))^2 ]   + \Var(\eps) \tag{$\EE[\eps] = 0$}\\
	&= \EE\Big[ (f(x_0) - \EE[\fhat(x_0)] + \EE[\fhat(x_0)]  - \fhat(x_0))^2 \Big]   + \Var(\eps) \\
	&= \EE\Big[ (f(x_0) - \EE[\fhat(x_0)])^2 + (\EE[\fhat(x_0)]  - \fhat(x_0))^2 + 2(f(x_0) \\ 
	& \qquad  - \EE[\fhat(x_0)])(\EE[\fhat(x_0)]  - \fhat(x_0)) \Big]   + \Var(\eps) \\
	&= \underbrace{\EE[(f(x_0) - \EE[\fhat(x_0)])^2]}_{\text{Bias squared}} + \underbrace{\EE[(\EE[\fhat(x_0)]  - \fhat(x_0))^2]}_{\text{Variance}} + \\
	& \qquad + \EE[2(f(x_0) - \EE[\fhat(x_0)])(\EE[\fhat(x_0)]  - \fhat(x_0))] + \Var(\eps)\\ 
	&= [\Bias(\fhat(x_0))]^2 + \Var(\fhat(x_0)) + 2\Big(f(x_0) - \EE[\fhat(x_0)] \Big) \underbrace{\EE\Big[\EE[\fhat(x_0)]  - \fhat(x_0)\Big]}_\text{$ = 0$} + \Var(\eps) \\
	&= [\Bias(\fhat(x_0))]^2 + \Var(\fhat(x_0)) + \Var(\eps) 	
	\end{align*}
	\qed
\item {\color{brown} For any point estimator $\hat\th$ of an estimand $\th$, we can define the mean squared error (MSE) which is $$\text{MSE}(\hat \th) = \EE[ ( \th - \hat \th)^2]$$ 
There is a similar decomposition of MSE into the sum of variance and bias squared.}
\item As a general rule, as flexibility increases in model selection, variance increases and bias decreases.  But the overall effect on test MSE depends on the relative rates of change of these quantities.  For a very flexible model, a small increase in flexibility gives a small decrease in bias, but a large increase in variance, thus we see an increase in test MSE.
\item {\color{brown} Since variance concerns variation in the model if we were to use different training sets, the smaller the training set is, the more variance we should expect to see.  Consequently, if we have a large training set, variance should be less of a concern.  In this case, we can afford to use more complicated models.}
\item Here is a typical image representing the state of affairs:
%\footnote{\url{https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9}} 

	\includegraphics[scale = 0.5]{bias_variance.png}
	
\item There is a ``trade-off'' since, at the two-extremes of an empty model where we always predict the grand mean of the response variable, and an overfitting model which perfectly interpolates the data, represent the extremes in variance and bias.  In the first case, we have very low variance (in fact, none) but high bias, and in the second case we see high variance and very low bias.

\item In real life, $f$ is typically unknown.  So it is generally not possible to compute test MSE, bias or variance.
\end{enumerate}

\vspace{.2in} 

\begin{enumerate}
\subsubsection{The Classification Setting} 
\item Suppose now that the response variable is qualitative, rather than quantitative.  We say we are in the classification setting.  We still want to estimate $f$.  How do quantify the accuracy of $\fhat$ in the classification setting?  We use the \textbf{error rate}, also called the \textbf{training error rate}:
$$\text{error rate} = \dfrac{1}{n} \sum_{i = 1}^n I(y_i \neq \hat{y}_i) $$
where $I$ is an \textbf{indicator variable}: 
$$I(\text{proposition}) = \begin{cases} 1 & \text{if proposition is TRUE} \\ 0 & \text{if proposition is FALSE}  \end{cases}$$
so that:
$$I(y_i \neq \hat{y}_i)  = \begin{cases} 
1 & \text{if } y_i \neq \hat{y}_i \\
0 & \text{if } y_i =  \hat{y}_i
\end{cases}$$
Therefore, $\sum I(y_i \neq \hat{y}_i)$ counts up the number of misclassifications.

\item The \textbf{test error rate} is the error rate over test observations.  A good classifier is one for which the test error rate is smallest.

\item The optimal classifier is the one which assigns each observation to the most likely class, given its predictor values.  That is, we predict that for an observation $x_0$ that $Y$ is class $j$ if:
$$P(Y = j | X = x_0)$$
is largest.  We call this the \textbf{Bayes classifier}. 
\item The Bayes classifer produces the lowest possible error rate.

\item Upon input $X = x_0$, the Bayes classifier will correctly classify with probability precisely $$\max_j P(Y = j | X = x_0).$$
Therefore, the error rate upon input $X = x_0$ of the Bayes classifier will be:
$$ 1 - \max_j P(Y = j | X = x_0)$$
The overall Bayes error rate is
$$ 1 - \EE\left[ \max_j P(Y = j | X = x_0) \right] $$
\item The overall Bayes error rate is analogous to irreducible error.
\item If we have a two-class or binary classification problem, then the Bayes classifier selects the class for which the conditional probability is greater than 0.5.
\item The values of $X$ which make the conditional probability equal to 0.5 is called the \textbf{Bayes decision boundary}.
\item Computing the Bayes classifier is impossible since we do not know the conditional distribution of $Y$ given $X$.  The Bayes classifier is the theoretically optimal model. Many approaches estimate the conditional distribution of $Y$ given $X$, then classify an observation using the highest estimated probability.  
\item One such method is the \textbf{$K$-nearest neighbors} or \textbf{KNN classifer}. Given a positive integer $K$ and a test observation $x_0$, the KNN-classifier finds the $K$ points in the training data which are nearest to $x_0$ represented by $\mathcal{N}_0$.  It then estimates the conditional probability as the fraction of the points in $\mathcal{N}_0$ whose response values equal $j$:
$$P(Y = j | X = x_0) = \dfrac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j) $$
The KNN classifier classifies the test observation  to be in the class $j$ with the highest probability.
\item In binary classification problems, there is a KNN decision boundary.  The goal is to find $K$ for which the KNN decision boundary is very close to the Bayes decision boundary.  
\item Small values of $K$ give a very flexible model, while very large values of $K$ are not flexible.  And so we see the bias-variance tradeoff at work in this family of models.
\end{enumerate}

\section{Linear Regression} 

\begin{enumerate}
	\item \textbf{Linear regression }is a very simple approach to supervised learning.
	\item It is a good jumping-off point for newer approaches.
	\item Some questions that linear regression can answer:
	\begin{itemize}
		\item Is there a relationship between a predictor and response variable?
		\item How strong is the relationship?
		\item Which subset of predictors are associated with response variable?
		\item How large is the association?
		\item How accurately can we predict response variable?
		\item Is the relationship linear?
		\item Is there \textbf{synergy} among predictors?
	\end{itemize}
\end{enumerate}

\vspace{.2in} 

\subsection{Simple Linear Regression} 
\begin{enumerate}
	\item This is for predicting a quantitative response variable $Y$ using a single predictor $X$.
	$$ Y \approx \beta_0 + \beta_1 X$$
	\item We can read ``$\approx$" as ``is approximately modeled as.'' 
	\item We sometimes say that we are \textbf{regressing $Y$ onto $X$}.
	\item We call $\beta_0$ the \textbf{intercept} and $\beta_1$ the \textbf{slope}. These are the model \textbf{coefficients} or \textbf{parameters.} 
	\item We use training data to produce estimates $\hat \beta_0$ and $\hat \beta_1$  We use these to produce an prediction for the value of $Y$:
	$$\hat y = \hat \beta_0 + \hat \beta_1 x$$
\end{enumerate}

\subsubsection{Estimating the Coefficients} 
\begin{enumerate}
	\item Let $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$ represent $n$ observation pairs.
	\item Using this data, we want to obtain coefficient estimates $\hat \beta_0$ and $\hat \beta_1$ so that $ y_i \approx \hat \beta_0 + \hat \beta_1 x_i$.  Here we are using ``$\approx$" in the numerical sense.
	\item There are a number of ways of measuring \textbf{closeness}, but the most common approach is to minimize the \textbf{least squares} criterion. Other approaches will be discussed later.
	\item  Let $\hat y_i =  \hat \beta_0 + \hat \beta_1 x_i$ be the prediction for $Y$ upon the $i$th value of $X$.  Then $e_i = y_i - \hat y_i$ is the $i$th \textbf{residual}.
	\item The \textbf{residual sum of squares} (RSS) is:
	$$\text{RSS} = e_1^2 + e_2^2 + \dots + e_n^2$$
	which is equivalent to:
	$$\text{RSS} = ( y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + ( y_2 - \hat \beta_0 - \hat \beta_1 x_2)^2 + \dots + ( y_n - \hat \beta_0 - \hat \beta_1 x_n)^2$$
	The least squares approach chooses $\hat \beta_0$ and $\hat \beta_1$ to minimize RSS.
	\item Let's minimize $RSS$:
	\begin{align*}
	0 = \pd{}{\hat \beta_0} \text{RSS} &= \pd{}{\hat \beta_0} \sum_{i = 1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_i)^2 \\  
	&= \sum_{i = 1}^n \pd{}{\hat \beta_0} (y_i - \hat \beta_0 - \hat \beta_1 x_i)^2 \\
	&= \sum_{i = 1}^n -2(y_i - \hat \beta_0 - \hat \beta_1 x_i) 
	\end{align*}
	which implies that $\hat \beta_0 = \overline{y} - \hat \beta_1 \overline{x}$.
	\begin{align*}
	0 = \pd{}{\hat \beta_1} \text{RSS} &= \pd{}{\hat \beta_1} \sum_{i = 1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_i)^2 \\  
	&= \sum_{i = 1}^n \pd{}{\hat \beta_1} (y_i - \hat \beta_0 - \hat \beta_1 x_i)^2 \\
	&= \sum_{i = 1}^n -2x_i(y_i - \hat \beta_0 - \hat \beta_1 x_i) \\
	&= \sum_{i = 1}^n -2x_i(y_i - (\overline{y} - \hat\beta_1 \overline{x}) - \hat \beta_1 x_i) \\
	&= \sum_{i = 1}^n -2x_i(y_i - \overline{y} - \hat\beta_1 (x_i - \overline{x}))
	\end{align*}
	This is equivalent to:
	\begin{align*}
	0 &= \sum_{i = 1}^n x_i(y_i - \overline{y} - \hat\beta_1 (x_i - \overline{x})) \\
	0 &= \sum_{i = 1}^n (x_i - \overline{x} + \overline{x}) (y_i - \overline{y} - \hat\beta_1 (x_i - \overline{x})) \\
	0 &= \sum_{i = 1}^n (x_i - \overline{x} )(y_i - \overline{y})  - \hat\beta_1(x_i - \overline{x})^2 + \overline{x}(y_i - \overline{y}) - \hat \beta_1 \overline{x}(x_i - \overline{x} ) \\
	\sum_{i = 1}^n \hat \beta_1 (x_i - \overline{x})^2 &=  \sum_{i = 1}^n (x_i - \overline{x} )(y_i - \overline{y}) + \overline{x} \underbrace{\left(\sum_{i = 1}^n y_i - \overline{y} \right)}_{= 0} - \hat\beta_1\overline{x} \underbrace{\left( \sum_{i = 1}^n x_i - \overline{x} \right)}_{ = 0} \\
	\hat\beta_1 &= \dfrac{ \sum_{i = 1}^n (x_i - \overline{x} )(y_i - \overline{y})}{\sum_{i = 1}^n (x_i - \overline{x} )^2}
	\end{align*}
	Thus, to minimize RSS, we have:
	\impbox{\textbf{Least squares regression coefficient estimates:} 
		\begin{align*}
		\hat\beta_1 &= \dfrac{ \sum_{i = 1}^n (x_i - \overline{x} )(y_i - \overline{y})}{\sum_{i = 1}^n (x_i - \overline{x} )^2} \\
		\hat \beta_0 &= \overline{y} - \hat \beta_1 \overline{x}
		\end{align*}}
\end{enumerate}

\vspace{.2in} 

\subsubsection{Assessing the Accuracy of the Coefficient Estimates} 
\begin{enumerate}
	\item Recall that our underlying assumption is that the true relationship between $X$ and $Y$ is of the form $$ Y = f(X) + \eps$$
	If $f$ is to be approximated by a linear function, then we can write this relationship as:
	$$Y = \beta_0 + \beta_1 X + \eps$$
	This defines the \textbf{population regression line}.
	\item Here, $\beta_0$ is the expected value of $Y$ when $X = 0$ (the intercept), and $\beta_1$ is the slope -- the average increase in $Y$ associated with a one-unit increase in $X$.  Here, the error term $\eps$ catches everything that is missed by this simple model.
	\item The \textbf{least squares line}, the one characterized by the above formulas for $\hat \beta_0$ and $\hat \beta_1$, is derived from a sample of data arising from the population.  Thus, there can be many different least squares lines.
	\item Recall that the sample mean is an \textbf{unbiased estimator} of the population mean.  This means that if you take the average over all possible samples of the sample mean, you will exactly obtain the population mean. In the same way, our estimates for $\beta_0$ and $\beta_1$ are unbiased estimators.
	\item Continuing the analogy, we can ask how accurate a sample mean $\hat \mu$ as an estimate of $\mu$.  We answer this question by computing the \textbf{standard error} of $\hat \mu$:
	$$\Var(\hat \mu) = {\rm{SE}}(\hat\mu)^2 = \dfrac{\sigma^2}{n}$$
	where $\sigma$ is the standard deviation of the population.  This roughly tells us the average amount that $\hat \mu$ differs from $\mu$.
	
	The standard errors associated with our estimates for $\beta_0$ and $\beta_1$ are:
	\impbox{ \textbf{Standard errors of least squares regression coefficient estimates:} 
	$${\rm{SE}}(\hat \beta_0)^2 = \sigma^2 \left[ \dfrac{1}{n} + \dfrac{\overline{x}}{\sum_{i = 1}^n (x _i - \overline{x} )^2 } \right] $$
	$$ {\rm{SE}}(\hat \beta_1)^2 = \dfrac{\sigma^2}{\sum_{i = 1}^n (x _i - \overline{x} )^2} $$
	where $\sigma^2 = \Var(\epsilon)$.
	}
	\item  {\color{brown}The derivation of these formulas will come later, when we study multiple linear regression. (see {\url{https://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression/44841#44841}} )}
	\item Notice that $\SE(\hat \beta_0)$ is equal to $\SE(\hat \mu)$ if $\overline{x} = 0$ (in which case, $\hat\beta_0 = \overline{y}$).
	\item Notice that $\SE(\hat \beta_1)$ is gets smaller the more spread out the $x_i$ gets.
	\item We do not know $\sigma^2$.  We can estimate it from the data.  The estimate is called \textbf{residual standard error}:
	$${\rm RSE} = \sqrt{\dfrac{{\rm RSS}}{n - 2}} $$
	When we use this estimate, the notation should be $\widehat{\rm SE}(\hat \beta_1) $ to indicate that an estimate has been made, but this hat will be dropped.
	\item Standard errors can be used to compute \textbf{confidence intervals}.  ``A 95\% confidence interval is defined as a range of values such that with 95\% probability, the range will contain the true unknown value of the parameter." {\color{brown} This statement seems wrong.  According to frequentist statistics, we cannot make a statement like this.  We do not treat $\beta_1$ as a random variable -- it is a fixed quantity so there is no probability distribution that we can use to describe $\beta_1$.}
	\item The 95\% confidence interval has the following property.  If we take different random samples, and construct the confidence interval for each sample, 95\% of the confidence intervals will contain the true, unknown value of the parameter.
	\item For linear regression, the 95\% confidence interval for $\beta_1$ is of the form:
	$$\hat \beta_1 \pm 2 \cdot  \SE(\hat \beta_1)$$
	Similarly for $\beta_0$.
	\item We can also use standard errors to do \textbf{hypothesis tests} on the coefficients.  We test the \textbf{null hypothesis}:
	$$H_0 \text{ : There is no relationship between $X$ and $Y$}.$$
	versus the \textbf{alternative hypothesis}.
	$$H_1 \text{ : There is some relationship between $X$ and $Y$}. $$
	\item We can rewrite these hypotheses mathematically:
	$$H_0 : \beta_1 = 0 $$
	$$H_1 : \beta_1 \neq 0$$
	\item To test the null hypothesis, we need to determine whether $\hat \beta_1$ is sufficiently far from 0 that we can be confident $\beta_1$ is non-zero.  ``Sufficiently far'' is determined by $\SE(\hat \beta_1)$ -- if $\SE(\hat \beta_1)$ is small, then even small values of $\hat \beta_1$ may suggest that $\beta_1 \neq 0$.  
	\item We compute a \textbf{$t$-statistic}, given by:
	$$ t = \dfrac{\hat \beta_1 - 0}{\SE(\hat \beta_1)} $$
	If we assume the null hypothesis, then we expect the distribution of $t$ to be a \textbf{$t$-distribution} with $n-2$ degrees of freedom.  We can then compute the probability of observing a number equal to $|t|$ or larger in absolute value, assuming $\beta_1 = 0$.  We call this the \textbf{$p$-value}.
	\item If the $p$-value is small, then we \textbf{reject the null hypothesis}, and thus conclude that there is a relationship between $X$ and $Y$.
\end{enumerate}

\vspace{.2in} 

\subsubsection{Assessing the Accuracy of the Model} 
\begin{enumerate}
	\item If we have rejected the null hypothesis, thus concluding that there is a relationship between $X$ and $Y$, then we want to know the extent to which the model fits the data.  For this, we use the residual standard error (RSE) and the $R^2$ statistic.
	\item Recall the RSS, the residual sum of squares: 
	$$RSS = \sum_{i = 1}^n e_i^2 = \sum_{i = 1}^n (y_i - \hat y_i)^2$$
	\item Recall also that from our regression model, associated with each observation is an error term $\eps$. The \textbf{residual standard error} or RSE is an estimate of the standard deviation of $\eps$.  Roughly, it is the average amount the response will deviate from the true regression line.  In the case of simple regression, RSE is computed as:
	$$\text{RSE} = \sqrt{\dfrac{1}{n - 2} \text{RSS}} = \sqrt{\dfrac{1}{n - 2} \sum_{i = 1}^n (y_i - \hat y_i)^2}$$
	There is a more general formula in the case of multiple linear regression.
	\item The RSE is considered a measure of the \textbf{lack of fit} of the model to the data.
	\item The $R^2$ statistic is also a measure of fit, but it is not measured in units of $Y$ and so it is more easily interpretable.  
	\item $R^2$ measures the proportion of variance explained.  It is always between 0 and 1.  We  define the \textbf{total sum of squares} (TSS) to be:
	$${\rm{TSS}} = \sum_{i = 1}^n (y_i - \overline{y})^2 $$
	(this is the error of the empty model).  In this case, the amount of error explained by the model is $TSS - RSS$.
	\item $R^2$ is defined as the proportion of TSS explained by the model:
	$$R^2 = \dfrac{{\rm{TSS}} - {\rm{RSS}}}{{\rm{TSS}}} =  1 - \dfrac{{\rm{RSS}}}{\rm{TSS}}$$
	\item We can also see $R^2$ as the amount of variance in $Y$ explained by $X$.
	\item It's hard to know what a ``good" $R^2$ is.  Obviously, the bigger the better, but different fields have different standards.  In fields like biology, psychology, marketing, an $R^2$ at 0.1 or below might be good.
	\item \textbf{{\color{brown} Sample }Correlation}, defined as:
	$$r = {\rm{Cor}}(X, Y) = \dfrac{\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sqrt{ \sum_{i = 1}^n (x_i - \overline{x})^2} \sqrt{\sum_{i = 1}^n (y_i - \overline{y})^2}}$$
	is also a measure of the linear relationship between $X$ and $Y$.  {\color{brown} How does this work?
	\begin{itemize}
		\item The denominator is something like the product of standard deviations.  
		\item The numerator is proportional to the \textbf{covariance} between $X$ and $Y$. Notice that if it tends to be the case that $y_i$ is large when $x_i$ is large (take large here to mean ``bigger than the mean value''), then the covariance is large and positive, and so we see a strong correlation.
		\item Notice that the units on covariance are $[X][Y]$, and correlation is dimensionless.
		\item One can think of correlation as being a normalization of covariance.
		\item Regarding goodness of fit, it is easy to show that if $y = mx + b$, then the correlation is $\rm{sgn}(m)$.
		\item It's also easy to show that if $X$ and $Y$ are independent, that is $\EE[XY] = \EE[X] \EE[Y]$, then $\rm{Cor}(X,Y) = 0$. 
		\item From the Cauchy-Schwartz inequality $|\langle u,v\rangle| \leq ||u||\cdot ||v||$, we can see that $r$ is between 0 and 1.
		\item There is also \textbf{population correlation}.  This is given by:
		$$\rho_{X, Y} = \dfrac{\Cov(X,Y)}{\sigma_X \sigma_Y}, \hspace{.2in} \text{where}\hspace{.2in} \Cov(X,Y) = \EE[X - \mu_X] \cdot \EE[Y - \mu_Y]$$
	\end{itemize}}
	\item In the context of simple linear regression, it turns out that $R^2 = r^2$.  This is not quite the case to multiple linear regression.
	
\end{enumerate}

\vspace{.2in} 

\subsection{Multiple Linear Regression} 

\begin{enumerate}
	\item If you have multiple predictors, one might think of running multiple simple regressions.  However, this is not entirely satisfactory as:
	\begin{itemize}
		\item it is unclear how to predict the outcome variable using multiple simple regressions.
		\item If there is correlations between predictors, this can lead to poor estimates of the outcome.
	\end{itemize}
	\item Instead, we generalize simple linear regression to accommodate $p$ predictors in a linear model.  This is called \textbf{multiple linear regression}. The model takes the form:
	$$ Y = \beta_0 + \beta_1 X_1 + \beta_2X_2 + \dots + \beta_pX_p + \eps$$
	We interpret $\beta_j$ as the average effect of $Y$ of a one unit increase in $X_j$ holding all other predictors fixed.
\end{enumerate}

\vspace{.2in} 
\subsubsection{Estimating the Regression Coefficients} 
\begin{enumerate}
	\item Just as before, we estimate $\beta_0, \dots \beta_p$, and make predictions using:
	$$\hat y = \hat \beta_0 + \hat \beta_1 x_1 + \dots + \hat \beta_p x_p$$
	\item We again use the least squares approach and choose $\hat \beta_0, \dots \hat \beta_p$ to minimize the sum of squared residuals: {\color{blue} TYPO in text page 72}
	$$ \text{RSS} = \sum_{i = 1}^n (y_i - \hat y_i)^2 = \sum_{i = 1}^n (y_i - \hat \beta_0 + \hat \beta_1 x_{i1} - \dots - \hat\beta_p x_{ip})^2 $$
	\item {\color{brown} The text does not give formulas or derivations for the estimates.  Their formulas are best presented using matrix algebra.  Computers can calculate these estimates easily in {\tt{R}}.  The text also glazes over the computation of standard errors of estimates, $t$-statistics, and $p$-values.}
	\item Situations such as the following can sometimes arise:
	\begin{itemize}
		\item A simple regression can show a statistically significant relationship between $Y$ and $X_1$.
		\item However, when multiple regression between $Y$ and $X_1, ... X_p$ is run, it shows that there is no significant relationship between $X_1$ and $Y$.  This might happen because there is a correlation between $X_1$ and some other predictor, $X_2$ say. And we may see that $X_2$ does have a significant relationship with $Y$. 
		\item In this case, in the simple regression, $X_1$ is getting credit for $X_2$'s effect on $Y$.  
		\item The classic example of this phenomenon is that ice cream sales are correlated with shark attacks at the beach.  These are correlated, 
	\end{itemize}
\end{enumerate}

\vspace{.2in} 
\subsubsection{Some Important Questions} 
\begin{enumerate}
	\item \textbf{1. Is there a relationship between the response and predictors?}  This is a hypothesis testing question:
	$$H_0 : \beta_1 = \beta_2 = \dots = \beta_p = 0$$
	$$H_a : \text{at least one of $\beta_j$ is non-zero.}$$
	\item We use the \textbf{$F$-statistic} to answer this question:
	$$F = \dfrac{(\text{TSS} - \text{RSS}) / p}{\text{RSS}/(n - p - 1)}$$
	If the linear model assumptions are correct, then one can show that the expected value of the denominator is $\sigma^2$.  And if $H_0$ is true, then the expected value of the numerator is $\sigma^2$.  Therefore, if the null hypothesis is true, we should expect $F = 1$.  And if $H_a$ is true, then we expect $\EE[(\text{TSS} - \text{RSS})/p] > \sigma^2,$ so we expect $F$ to be greater than 1. 
	\item {\color{brown} I would like to see a justification for the two statements above.  
		
		\begin{itemize}
			\item Maybe this is unrelated, but I have seen in several places that $$\text{TSS} = \text{MSS} + \text{RSS}$$ where:
			$$\text{TSS} = \sum_{i = 1}^n (y_i - \overline{y})^2, \hspace{.1in} \text{MSS} = \sum_{i = 1}^n (\hat y_i - \overline{y})^2, \hspace{.1in} \text{RSS} = \sum_{i = 1}^n (y_i - \hat y_i )^2$$
			This is not obvious!  Let's prove it for simple linear regression, though I believe it holds for multiple linear regression as well.
			
			\noindent \textbf{Proof:} $$y_i - \overline{y} = (y_i - \hat y_i) + (\hat y_i - \overline{y}) $$
			Now square both sides and sum:
			$$\text{TSS} = \text{MSS} + \text{RSS} + \sum_{i = 1}^n 2(y_i - \hat y_i)(\hat y_i - \overline{y})$$
			We want to show the last sum is 0.  By simple linear regression, we know that:
			$$\hat y_i = \hat\beta_0 + \hat\beta_1 x_i$$
			where 
			$$\hat \beta_0 = \overline{y} - \hat\beta_1\overline{x}, \hspace{.2in} \text{and} \hspace{.2in} 
			\hat\beta_1 = \dfrac{ \sum_{i = 1}^n (x_i - \overline{x} )(y_i - \overline{y})}{\sum_{i = 1}^n (x_i - \overline{x} )^2}$$
			Therefore:
			\begin{align*}
			\sum_{i = 1}^n (y_i - \hat y_i)(\hat y_i - \overline{y}) &= \sum_{i = 1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_i)(\hat \beta_0 + \hat \beta_1 x_i - \overline{y}) \\
			&= \sum_{i = 1}^n (y_i - \overline{y} + \hat\beta_1 \overline{x}  - \hat \beta_1 x_i)(\overline{y} - \hat\beta_1\overline{x} + \hat \beta_1 x_i - \overline{y}) \\
			&= \sum_{i = 1}^n \Big((y_i - \overline{y}) - \hat\beta_1 (x_i - \overline{x})  \Big)\Big( \hat\beta_1(x_i -\overline{x})\Big) \\
			&= \sum_{i = 1}^n \hat\beta_1 (x_i - \overline{x})(y_i - \overline{y}) - \hat\beta_1^2(x_i - \overline{x})^2 \\
			&= \hat\beta_1 \left[ \sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})  - \hat\beta_1 \sum_{i = 1}^n (x_i - \overline{x})^2 \right] \\ 
			&= \hat\beta_1 \left[ \sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})  - \dfrac{ \sum_{i = 1}^n (x_i - \overline{x} )(y_i - \overline{y})}{\sum_{i = 1}^n (x_i - \overline{x} )^2} \sum_{i = 1}^n (x_i - \overline{x})^2 \right] = 0\\
			\end{align*}
			\qed
			
			
			\item What are the linear model assumptions?  This is that the errors $\eps$ are independently and identically distributed with mean 0 and variance $\sigma^2$.
			\item The numerator of $F$ is referred to as the \textbf{MSM, mean square model}.  Notice that:$$\text{MSM} = \dfrac{\text{MSS}}{p}$$
			we should view $p$ as the degrees of freedom in the model.  We can see this arise if we examine the degrees of freedom in the alternate realization of $\text{MSS}$ as $\text{TSS} - \text{RSS}$.  The degrees of freedom of $\text{TSS}$ is $n - 1$, and the degrees of freedom remaining after incorporating $p$ predictors is $n - p - 1$.    The difference of these is $p$.
			
			\item The denominator is referred to as the \textbf{MSE, or mean squared error}.  Notice that: 
			$$\text{MSE} = \dfrac{\text{RSS}}{n - p - 1}$$
			which is in line with what we have above -- sum of squared deviation divided by degrees of freedom.
			
			\item As an aside to this aside, the reason we divide by these degrees of freedoms is to get unbiased estimators, meaning, in this case, that the expected value of MSM and MSE should both be $\sigma^2$.
			
			\item I'm starting to realize that this unbiased estimator thing is going to require a tremendous amount of work.  For this reason, I will build up a separate document.  I think I also should understand multiple linear regression's exposition in terms of matrix algebra before really attempting to understand why MSS/$p$ and RSS/$(n - p - 1)$ are unbiased -- I might be able to prove it for $p = 1$ though.
		\end{itemize}
	}
	\item How large does the $F$-statistic need to be before we reject $H_0$?  This depends on $n$ and $p$.  When $n$ is large, $F$ can be just a bit larger than 1 to obtain a sufficiently small $p$-value.  
	\item When $H_0$ is true and the errors $\eps_i$ are normally distributed, the $F$-statistic follows an $F$-distribution.
	\item {\color{brown} There is no discussion about the $F$-distribution, or details of computing the $p$-value of a $F$-statistic in the text. The text does say that {\tt R} can do it.}
	\item Suppose we want to show that a subset of the coefficients are 0.  This corresponds to a null hypothesis:
	$$H_0 : \beta_{p - q + 1} = \beta_{p - q + 2} = \dots = \beta_p = 0$$
	A second model is fit which uses all the predictors except these $q$, and a residual sum of squares $\text{RSS}_0$ is computed.  This quantity $\text{RSS}_0$ is used in place of $\text{TSS}$ in the computation of the $F$-statistic (when $q = p$, this second model is the empty model).  That is:
	$$F = \dfrac{(\text{RSS}_0 - \text{RSS})/q}{\text{RSS}/(n - p - 1)}$$
	\item One can use this to test the significance of each individual predictor while holding the others fixed.  In fact, the $F$-statistic so computed is the square of a $t$-statistic associated to the predictor in the multiple linear regression.
	\item Here is an example relating three predictors to a response variable called {\tt sales}.
	 
	\includegraphics[scale = 0.3]{multiple_regression_example.png}
	
	If we were to compute the $F$-statistic where $q = 1$ and we omit {\tt newspaper}, then the resulting value would be $(-0.18)^2$.
	\item It is important in hypothesis testing to use the $F$-statistic instead of relying on the $p$-values in the above table for each individual predictor, particularly when the number of predictors $p$ is large.  This is because we can achieve a small enough $p$-value ($< 0.05$) simply by chance alone with a large number of predictors.  In light of this, it would be foolish to reject the null hypothesis with this sort of method.
	
	\item If $p > n$, then there are more coefficients to estimate than observations from which to estimate them.  In this case, multiple linear regression using least squares does not make sense.  This situation is referred to as a \textbf{high-dimensional} setting, as is discussed later in the text.	
	\item \textbf{2. Which variables are important?} If we decide to reject the null hypothesis, it is natural to wonder which of the predictors is related to the response.  The task of determining which predictors are associated with the response in order to fit a single model involving only those predictors is referred to as \textbf{variable selection.}
	\item As we discussed, it would be foolish to look at individual $p$-values for the predictors, especially when the number of predictors is large.
	\item We might try constructing the $2^p$ possible models using one of the $2^p$ subsets of the predictors.  But how do we determine which model is best?  There are various statistics which can be used to judge the quality of a model -- these will be discussed in Chapter 6. However, $2^p$ grows quickly with $p$, thus rendering this strategy infeasible.
	\item There are three classical approaches to select models to consider:
		\begin{itemize}
			\item \textbf{Forward selection:} Start with the \textbf{null model} (aka empty model).  Perform the $p$ simple linear regressions and add to the null model the variable with the lowest RSS.  Then do $p-1$ two-variable regressions where one of the other variables is added.  Keep going until some stopping rule is satisfied.
			\item \textbf{Backward selection:} Start with all variables, then remove the variable with the largest $p$-value.  Fit the new $(p-1)$-variable model, then remove the largest $p$-value variable.  Continue until a stopping rule is reached -- perhaps stop when all variables have a $p$-value below a threshold.
			\item \textbf{Mixed selection:} Start with no variables.  Add the variable of best fit.  Continue to add variables, and if the $p$-value for any of the variables rises above a threshold, remove it.  Continue until all variables in the model have low $p$-value, and all other variables would have a give large $p$-values if added.		 
		\end{itemize}
	\item \textbf{3. Model Fit} The two most common numerical measures of model fit are RSE and $R^2$.
	\item We saw that in simple regression of $Y$ onto $X$, $R^2$ turns out to be the square of the correlation between $X$ and $Y$.  In multiple linear regression, it turns out that $$R^2 = \text{Cor}(Y, \hat Y)^2$$
	{\color{brown} \theorem $\Cor(Y, \hat Y) = |\Cor(X,Y)|$ in the case of simple regression $\hat Y = \hat \beta_0 + \hat \beta_1 X$. 
	
	\noindent \textbf{Proof:} 
	\begin{align*}
	\Cor(Y, \hat Y) = \Cor(Y, \hat \beta_0 + \hat \beta_1 X) &= \dfrac{\displaystyle\sum_{i = 1}^n (y_i - \overline{y}) (\hat\beta_0 + \hat\beta_1 x_i - (\overline{\hat\beta_0 + \hat\beta_1 x_i})}
	{\sqrt{\displaystyle \sum_{i =1}^n (y_i - \overline{y})^2}\sqrt{\displaystyle\sum_{i = 1}^n (\hat\beta_0 + \hat\beta_1x_i - (\overline{\hat \beta_0 + \hat\beta_1 x_i})^2}} \\
	&= \dfrac{\displaystyle\sum_{i = 1}^n (y_i - \overline{y}) (\hat\beta_0 + \hat\beta_1 x_i - \hat\beta_0 - \hat\beta_1 \overline{x})}
	{\sqrt{\displaystyle \sum_{i =1}^n (y_i - \overline{y})^2}\sqrt{\displaystyle\sum_{i = 1}^n (\hat\beta_0 + \hat\beta_1x_i - \hat \beta_0 - \hat\beta_1 \overline{x})^2}}\\
	&= \dfrac{\displaystyle\sum_{i = 1}^n (y_i - \overline{y}) (\hat\beta_1 (x_i - \overline{x}))}
	{\sqrt{\displaystyle \sum_{i =1}^n (y_i - \overline{y})^2}\sqrt{\displaystyle\sum_{i = 1}^n  \hat\beta_1^2(x_i - \overline{x})^2}} \\
	&= \dfrac{\hat\beta_1}{|\hat\beta_1|} \cdot \dfrac{\displaystyle\sum_{i = 1}^n (y_i - \overline{y}) (x_i - \overline{x})}
	{\sqrt{\displaystyle \sum_{i =1}^n (y_i - \overline{y})^2}\sqrt{\displaystyle\sum_{i = 1}^n  (x_i - \overline{x})^2}} = \text{sgn}(\hat\beta_1) \cdot \Cor(X,Y)
	\end{align*}
	\qed 
	}
	\item Every additional variable will decrease $R^2$. However, if adding a variable leads only to a small increase in $R^2$, then we run into the problem of potential overfitting -- the model may be picking up on noise.
	\item Recall that the RSE, the \textbf{residual standard error} is an estimate of the standard deviation of the residual.  In general, RSE is defined as:
	$$\text{RSE} = \sqrt{\dfrac{1}{n - p -1 } \text{RSS}}$$
	\item Adding a variable to a model will always cause RSS to decrease.  However, RSE may nonetheless increase since the denominator gets smaller.
	\item \textbf{4. Prediction} The multiple regression model allows us to make predictions on the response variable using a values for the predictors $X_1, ... X_p$.  Where does the uncertainty in our prediction $\hat Y$ lie?
	\begin{itemize} 
	\item The coefficient estimates is a form of reducible error.  We compute a confidence interval to account for this error.
	\item Assuming a linear model is an additional source of potentially reducible error called \textbf{model bias}.
	\item There is also the irreducible error $\eps$.  To account for this, we use a \textbf{prediction interval}.  This is an interval which incorporates both the reducible error in our coefficient estimates, as well as the irreducible error. 
	\item The confidence interval is used to quantify the uncertainty surrounding the average value of $f(X)$ -- if we collect a large number of data sets with observations in $X$, and construct confdience intervals for each, then 95\% of these will contain the true value of the average of $f(X)$.  The prediction interval is an estimate of the uncertainty around a single predicted value.  It provides a range that we expect a new observation to fall within given the values of the predictors.  It not only captures uncertainty in the parameters (like the confidence interval), but also random error in the observation.
	\end{itemize} 
	\item {\color{brown} The text discusses confidence intervals and prediction intervals for a particular set of inputs.  It gives no indication of how these were constructed. }
	\end{enumerate}

\vspace{.2in} 

\subsection{Other Considerations in the Regression Model} 
\subsubsection{Qualitative Predictors} 
\begin{enumerate}
	\item Another term for a qualitative predictor is \textbf{factor.} A factor can have \textbf{levels}, or possible values.  
	\item If a factor has two levels, (for example FALSE and TRUE)then we can create an \textbf{indicator} or \textbf{dummy variable} which is either 0 or 1 depending on the value of the factor.  
	\item We then use this in a regression model:
	$$y_i = \beta_0 + \beta_1 x_i + \eps_i = \begin{cases} \beta_0 + \beta_1 + \eps_i & \text{if $x_i =$ TRUE} \\
	\beta_0 + \eps_i & \text{if $x_i =$ FALSE} \end{cases}$$
	\item Here, $\beta_0$ is the average value of $Y$ among those observations for which $X$ is FALSE.  \item And $\beta_1$ is the average difference in values in $Y$ between those observations for which $X$ is TRUE and $X$ is FALSE.
	\item The encoding of the dummy variable is arbitrary.  Changing the values of $x_i$ to be $-1$ and $1$, say, will change the interpretation of the resulting regression coefficients, but will not change the predictions.
	\item If a factor has more than two levels, then we use additional dummy variables.  For example, if $X$ can take values \{For (F), Neutral (N), Against (A)\}, then you need two dummy variables.  Perhaps:
	$$x_{i1} = \begin{cases} 1 & \text{if $X =$ F} \\
	0 & \text{if $X \neq$ F} \end{cases}  $$
	and 
	$$x_{i2} = \begin{cases} 1 & \text{if $X =$ A} \\
	0 & \text{if $X \neq$ A} \end{cases}  $$
	Then our regression model:
	$$y_i = \beta_0 + \beta_1x_{i1} + \beta_2x{i2} + \eps_i = \begin{cases} \beta_0 + \beta_1 + \eps_i & \text{if $X =$ F} \\
	\beta_0 + \beta_2 + \eps_i & \text{if $X =$ A} \\
	\beta_0 + \eps_i & \text{if $X =$ N} \end{cases} $$
	\item There will always be one fewer dummy variable than the number of levels.
	\item The level for which all dummy variables are 0 is known as the \textbf{baseline}. 
	\item There is no issue with mixing quantitative and qualitative variables in a regression model.
\end{enumerate}

\vspace{.2in} 

\subsubsection{Extensions of the Linear Model}
\begin{enumerate}
	\item Linear regression assumes that the relationship between predictors and response is both \textbf{additive} and \textbf{linear}. Additive means that the relationship between a predictor $X_j$ and response $Y$ does not depend on any of values of the other predictors.  Linear means that the change in the response $Y$ associated with a one-unit change in $X_j$ is constant, regardless of the value of $X_j$.  
	\item Suppose there is a \textbf{synergy} (aka \textbf{interaction}) between predictors $X_1$ and $X_2$, meaning that the value of, say, $X_2$ can have an effect on the relationship of $X_1$ on $Y$.
	\item For example, suppose you have a fixed \$100,000 budget to spend on advertising.  You fit the response {\tt sales} with predictors {\tt tv} and {\tt radio}. A regression model would imply that to maximize {\tt sales}, one should plunge the full budget into the media (tv or radio) with the higher coefficient -- let's say that's tv.  However, it could be the case that spending half on radio and half on tv increases sales more than giving the entire amount to tv.  This is an interaction effect. 
	\item One way to extend the linear model to include interactions is to include \textbf{interaction terms} constructed by computing the product of $X_1$ and $X_2$:
	$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2 + \eps$$
	In this model, changing the value of $X_2$ will change the association between $X_1$ and $Y$.
	\item In regression, non-interaction terms are called \textbf{main effects}. 
	\item It can be the case that an interaction term has a small $p$-value, but the main effects do not.  However, the \textbf{hierarchical principle} states that if an interaction term is included in a model, then the model should also include the main effects, even if the $p$-values associated with their coefficients are not significant. {\color{brown} I don't fully understand the rationale for why this principle should be adopted and I don't find the following to be very convincing.} The rationale for this principle is that if $X_1X_2$ is related to the response, then whether or not the coefficients of the main effects are zero is of little interest.
	\item Interaction terms can involve qualitative variables as well -- this is straightforward.
	\item Non-linear relationships can be fit using \textbf{polynomial regression}.  For example, we can fit a model:
	$$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \eps$$
	This is still a linear model.  
\end{enumerate}
\vspace{.2in} 

\subsubsection{Potential Problems} 
\begin{enumerate}
	\item Many problems may occur when fitting a linear regression model.
	\begin{itemize}
		\item \textbf{Non-linearity of the response-predictor relationships.}  A good way of identifying non-linearity are \textbf{residual plots} which plot $e_i = y_i - \hat y_i$ versus the predictor $x_i$.  The presence of a pattern in a residual plot may indicate a problem with a linear assumption.
		\item \textbf{Correlation of error terms}.  If there is correlation among error terms, then estimated standard errors will tend to underestimate true standard errors, which leads to a narrowing of confidence and prediction intervals.  This also leads to a smaller $p$-value.  A good example of this is if we were to double all observations increasing the sample size to $2n$.  Estimated parameters would not change, but confidence intervals would be narrower by a factor of $\sqrt{2}$. {\color{brown} I don't really understand the problem with this.  If the data is doubled, and we had two observations for each data point, isn't it sensible that standard error is reduced?  ChatGPT agrees with me for what that's worth.}
		
		Correlations between error terms frequently occur in \textbf{time series} data, which consists of observations at discrete points in time.  Correlations can be made evident by plotting residuals against time.  Correlation may appear as adjacent observations have correlated error terms -- this is called \textbf{tracking}.
		
		Pictured below is an example of correlated error terms at different levels of corrleation.
		
		\includegraphics[scale = 0.3]{correlation_of_error_time_series.png}
		\item \textbf{Non-constant variance of error terms} Also known as \textbf{heteroscedasticity}.  Can be detected in a funnel shape in the residual plot.  When possible solution is to transform the response using a concave function: $\log(Y) or \sqrt{Y}$ for instance -- this shrinks larger values in response which leads to a reduction in heteroscedasticity.
		
		\includegraphics[scale = 0.3]{heteroscedasticity.png}
		
		Sometimes, variance in response is well understood.  For instance, if $Y_i$ is the average of $n_i$ observations, each of which has known variance $\sigma^2$, then $Y_i$ is known to have variance $\dfrac{\sigma^2}{n_i}$.  Then a simple remedy is to model by \textbf{weighted least squares} -- in this example, make weights $w_i = n_i$ so so to make the error constant.
		\item \textbf{Outliers}  An \textbf{outlier} is a point for which $y_i$ is far from $\hat y_i$.  Again, residual plots make outliers identifiable though in practice it can be difficult to decide how large a residual can be before it is considered an outlier.  Thus, we can instead plot \textbf{studentized residuals} which divides each residual $e_i$ by its estimated standard error.  We then use the rule that responses which are greater than 3 in absolute standardized residual are outliers.
		
		Outliers may indicate a missing predictor.
		\item \textbf{High-leverage points} are observations with an unusual value for predictors.  Note that even if an observation has values for predictors which fall within the ranges of predictors, the conjunction of values for the predictors can cause an observation to be a high leverage point.  We can quantify an observations leverage using the \textbf{leverage statistic}.  For simple linear regression, this is:
		$$h_i = \dfrac{1}{n} + \dfrac{(x_i - \overline{x})^2}{\sum_{j = 1}^n (x_j - \overline{x})^2}$$
		The average value of this statistic is $2/p$, so if an observation has a leverage much greater than $2/p$, we may suspect a high-leverage point.
		
		High leverage points have a large effect on the regression line.
		
		
		\item \textbf{Collinearity} refers to when predictor variables are highly correlated.  This is a problem for regression since it can be difficult to tease apart the association between correlated variables and the response.
		
		Imagine that $X_1$ and $X_2$ are highly correlated.  Then for a fixed value of RSS, there is a large range of possible values of $\hat \beta_1$ and $\hat \beta_2$ which can give the value of RSS.  Another way to say this is that there is a large standard error on these coefficient estimates.  This causes the $t$-statistic to decrease, which causes the $p$-value to grow and confidence intervals to widen.  This in turn makes it harder to reject the null hypothesis in regression -- that is, the \textbf{power} of the hypothesis test which is the probability of correctly detecting a non-zero coefficient is reduced by collinearity.
		
		Collinearity can be detected by looking at the correlation matrix of predictors.  However, it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation.  This is called \textbf{multicollinearity}. 
		
		To assess multicollinearity, we use the \textbf{variance inflation factor} (VIF).  This is the ratio of the variance of $\hat\beta_j$ when fitting the full model divided by the variance of $\hat \beta_j$ if fit on its own.  The smallest possible value for VIF is 1, which is the complete absence of collinearity.
		
		{\color{brown} This implies that the variance of $\hat \beta_j$ is larger when fitting the full model than when fitting the simple model.  Why is this so?  What is the variance of $\hat \beta_j$ anyway?  ChatGPT says that the variance of $\hat\beta_j$ is:
		$$\Var(\hat \beta_j) = \dfrac{\sigma^2}{\sum x_j^2}$$  I'm really getting the feeling that I need to learn multiple linear regression.}

		The VIF for each variable can be computed using the formula:
		$$\mathrm{VIF}(\hat\beta_j) = \dfrac{1}{1 - R_{X_j | X_{-j}}^2} $$
		where $R_{X_j | X_{-j}}^2$ is the $R^2$ from a regression of $X_j$ onto all the other predictors.  If $R_{X_j | X_{-j}}^2$ is close to one, then collinearity is present, so the VIF will be large.

		When collinearity is present, some solutions might be to drop one of the variables, or to create a new variable which is a mixture of collinear variables.  
	\end{itemize}
\end{enumerate}

\vspace{.2in} 

\subsection{The Marketing Plan}

\vspace{.2in} 

\subsection{Comparison of Linear Regression with $K$-Nearest Neighbors}
\begin{enumerate}
	\item In \textbf{$K$-nearest neighbors regression} (KNN regression), given a value for $K \in \mathbb{N}$ and a prediction point $x_0$, KNN regression considers the $K$ training observations that are closest to $x_0$ -- denote this set by $\mathcal{N}_0$.  Then: \begin{align*}
		\hat f(x_0) = \dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} y_i 
	\end{align*}
	\item KNN regression is a non-parametric method.
	\item Notice that if $K = 1$, then KNN regression gives perfect interpolation.  As $K$ increases, we get a smoother and less variable fit since the prediction is an average of several points.  However, this smoothing may cause bias by masking some of the structure in $f(X)$.
	\item The parametric approach will outperform the non-parametric approach if the parametric form that has been selected is close to the true form of $f$.  In this case, the non-parametric approach incurs a cost in variance which is not offset by a reduction in bias.
	\item The more non-linear the relationship between predictors and output is, the better KNN will perform.  On the other hand, linear regression may still win out when the number of predictors is large.  This is because of the \textbf{curse of dimensionality.} When data is distributed in a high-dimensional space, distances between data points tend to equalize.  
	\item {\color{brown} One way to think about this is to examine the ratio of the volume of a spherical ball $D^n$ of radius $r$ to the volume of an $n$-cube $I^n$ with edgelength $2r$.  We have:
	\begin{align*}
		\mathrm{Vol}(D^n) = \dfrac{2r^n \pi^{n/2}}{n \Gamma(n/2)}
	\end{align*}
	And so:
	\begin{align*}
		\lim_{n \to \infty} \dfrac{\mathrm{Vol}(D^n)}{\mathrm{Vol}(I^n)} = \lim_{n \to \infty} \dfrac{\pi^{n/2}}{n 2^{n - 1} \Gamma(n/2)} = 0
	\end{align*}
	So if we choose data randomly sampled from $I^n$, when $n$ is large, these tend to fall outside the spherical ball.
	
	But they also tend to stay away from the corners!  In fact, the volume of the cube is concentrated around a sphere of radius $\sqrt{n}/3$.  To show this, let's consider the average value of $x_i^2$ where $x_i$ is a  coordinate of a point in the cube of edgelength 2:
	\begin{align*}
		\EE[x_i^2] &= \dfrac{1}{2} \int_{-1}^1 x^2 \, dx \\
		&= \dfrac{1}{2} \left[ \dfrac{1}{3} x^3 \Big|_{-1}^1 \right] \\
		&= \dfrac{1}{3} 	
	\end{align*}
	and the variance:
	\begin{align*}
		\mathrm{Var}(x_i^2) &= \dfrac{1}{2} \int_{-1}^1 x^4 \, dx - (\EE[x_i^2])^2 \\
		&= \dfrac{1}{2} \cdot \dfrac{2}{5} - \dfrac{1}{9} = \dfrac{4}{45}  
	\end{align*}
	Therefore, the squared distance of a point from the origin $r^2 = \sum_i^n x_i^2$ has average value $n/3$ and variance $4n/45$.  We can apply the CLT to the average $r^2 / n$ (which is analogous to the sample mean) to conclude that this is close to a normal distribtuion with mean $1/3$ and standard deviation $2/\sqrt{45n}$.  Thus, most of the volume of the cube is concentrated near the sphere of radius $\sqrt{n/3}$.
	}

\end{enumerate}

\vspace{.2in} 

\section{Classification} 
\begin{enumerate}
	\item \textbf{Classification} refers to approaches for predicting qualitative or categorical responses.  A particular classfication technique is called a \textbf{classifier}. 
\end{enumerate}
\vspace{.2in} 

\subsection{An Overview of Classification} 
\begin{enumerate}
	\item Classification problems occur often, maybe even more so than regression problems.
	\item Some examples: \begin{itemize}
		\item A person visits the doctor exhibiting a set of symptoms which might point to one of three different diagnoses.  Which of the three conditions does the individual have?
		\item An online banking service must determine if a transaction is fraudulent on the basis of a user's IP address, past transaction history, and so forth.
		\item Which DNA mutations are deleterious (disease-causing), and which are not.
		\item Will a credit card holder default {\tt Default} on their payments?  We will base our prediction on annual income {\tt income} and card balance {\tt balance}.  This is the problem that will be highlighted in this chapter.
	\end{itemize}
\end{enumerate}

\vspace{.2in} 

\subsection{Why Not Linear Regression?}
\begin{enumerate}
	\item Linear regression is not appropriate in the case of a qualitative response.  Two main reasons: \begin{itemize}
		\item Regression methods cannot accommodate a qualitative response with more than two classes since these classes must be coded and this coding can be arbitrary.
		\item A regression method will not provide meaningful probability estimates when $Y$ is a variable with two classes.
	\end{itemize}
\end{enumerate}

\vspace{.2in} 

\subsection{Logistic Regression} 
\begin{enumerate}
	\item \textbf{Logistic regression} models the probability that $Y$ -- a two valued response variable -- takes on a particular value.
	\item In the example of {\tt default}, logistic regression models the probability of default given a value of {\tt balance}.  We will abbreviate $P(\tt default | \tt balance)$ by $p(\tt balance)$.
	\item One might predict $\tt default = \tt Yes$ for any individual for whom $p(\tt balance) > 0.5$.  Though one could be more conservative and put the threshold at $0.1$ instead.
\end{enumerate}
\subsubsection{The Logistic Model} 
\begin{enumerate}
	\item In general, in logistic regression, we want to find a relationship between $X$ and $p(X) = P(Y = 1| X)$.  Here we are using a generic $0/1$ coding for the values of $Y$.
	\item If we were to use linear regression:
	\begin{align*}
		p(x) = \beta_0 + \beta_1 X
	\end{align*}
	we may very well obtain values outside the range of $[0,1]$ that we want for probabilities, which is a problem.
	\item In logistic regression, we use the \textbf{logistic function} to model probabilities:
	\begin{align*}
		p(X) = \dfrac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
	\end{align*}
	As we let $X$ range, this produces an $S$-shaped curve ranging between 0 and 1.
	\item We can manipulate the logistic function to obtain:
	\begin{align*}
		\dfrac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1 X }
	\end{align*}
	The expression on the left is called the \textbf{odds}.
	\item If we take the log of both sides, we obtain:
	\begin{align*}
		\log\left(\dfrac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1X 
	\end{align*}
	The left hand side is called the \textbf{log odds} or \textbf{logit}.  
	\item We conclude that the logistic regression model has a logit which is linear in $X$.
	\item Increasing $X$ by 1 unit changes the log odds by $\beta_1$.  This corresponds to multiplying the odds by $e^{\beta_1}$.  
\end{enumerate}
\subsubsection{Estimating the Regression Coefficients} 
\begin{enumerate}
	\item Using training data, we will estimate $\beta_0$ and $\beta_1$ in the logistic regression model.  
	\item To do this, we do not use least squares (though we could).  We instead use a method known as \textbf{maximum likelihood} since it has better statistical properties.
	\item {\color{brown} Maximum likelihood estimation is a huge subject used all over statistics whenever a parameter estimate is required.  The general idea is to maximize a likeihood function so that, under the assumed statistical model, the observed data is most probable. 
	
	The mathematical details are suppressed in the text.  I have looked for texts which might shed some light -- Casella and Berger {\it{Statistical Inference}} comes highly receommended.}
	\item We can use {\tt R} to compute MLE for $\beta_0$ and $\beta_1$ in logistic regression.
	\item We typically don't care too much about the significance of $\beta_0$ since it's main purpose is to adjust the average fitted probabilities to the proportion of ones in the data.
	\item The significance of $\beta_1$ is of interest.  We can use a $z$-statistic and associated $p$-value to do hypothesis testing on whether $\beta_1 = 0$.  Of course, if the null hypothesis is rejected, this suggests there is a relationship between $X$ and $p(X)$.  	
\end{enumerate}
\subsubsection{Making Predictions} 
\begin{enumerate}
	\item Predictions are done in the obvious way once estimates $\hat \beta_0$ and $\hat \beta_1$ are computed -- just plug in the desired value for $X$.  
	\item Qualitative predictors can be used in logistic regression.  For example, if we wanted to know the probability of a student defaulting vs a non-student defaulting, we could do the usual dummy variable trick of coding a student as 1 and a non-student as 0 and computing the logistic regression model as usual.
\end{enumerate}
\subsubsection{Multiple Logistic Regression} 
\begin{enumerate}
	\item To predict a binary response using multiple predictors, we generalize:
	\begin{align*}
		\log\left( \dfrac{p(x)}{1 - p(X)} \right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X
	\end{align*}
	As before, we use MLE to estimate the $\beta$'s. 
	\item In the running example, we can fit logistic regression to {\tt default} on {\tt balance}, {\tt income}, and {\tt student} status.  A funny thing happens here.  \begin{itemize}
		\item When you do logistic regression using {\tt student} only, you get a positive coefficient which implies that students default at a higher rate than non-students. 
		\item However, in this multiple logistic regression, we obtain a \emph{negative} coefficient for {\tt student}.  This implies that for a fixed value of {\tt balance} and {\tt income}, the model predicts students default at rates lower than non-students.  
		\item Running simple logistic regression of {\tt default} against {\tt balance} shows that the higher the balance, the higher the default rate.
		\item It turns out that {\tt student}  is positively correlated with {\tt balance}.  In other words, students tend to have higher balances.  Thus, a higher proportion of students default because they tend to have higher balances.
		\item This sort of phenomenon is called \textbf{confounding.} 
	\end{itemize}
\end{enumerate}
\subsubsection{Multinomial Logistic Regression} 
\begin{enumerate}
	\item \textbf{Multinomial logistic regression} extends logistic regression to response variables with $K > 2$ classes.
	\item We first select a class to serve as the \textbf{baseline}.  WLOG, this will be the $K$th class.
	\item The model is:
	\begin{align*}
		P(Y = k| X = x) &= \dfrac{e^{\beta_{k0} + \beta_{k1} x_1 + \dots + \beta_{kp}x_p}}{1 + \sum_{l = 1}^{K - 1} e^{\beta_{l0} + \beta_{l1}x_1 + \dots + \beta_{lp}x_p }}, \hspace{.2in} k = 1, \dots, K-1 \\
		P(Y = K| X = x) &= \dfrac{1}{1 + \sum_{l = 1}^{K - 1} e^{\beta_{l0} + \beta_{l1}x_1 + \dots + \beta_{lp}x_p }}
	\end{align*}
	This is clearly a probability distribution on $Y = 1, ..., K$. 
	\item This model has $(p + 1)(K - 1)$ parameters.
	\item It is easy to show that the log-odds between any pair of classes is linear in the features.  For instance: 
	\begin{align*}
		\log\left( \dfrac{P(Y = k| X  = x)}{P(Y = K| X = x )}\right) = \beta_{k0} + \beta_{k1}x_1 + \cdots + \beta_{kp}x_p, \hspace{.2in} k = 1, 2, \dots, K-1
	\end{align*}
	and for $k \neq K$ and $j \neq K$:
	\begin{align*}
		\log\left( \dfrac{P(Y = k| X  = x)}{P(Y = j| X = x )}\right) 
		&= \log\left(\dfrac{\exp(\beta_{k0} + \beta_{k1}x_1 + \cdots + \beta_{kp}x_p)}{\exp(\beta_{j0} + \beta_{j1}x_1 + \cdots + \beta_{jp}x_p)} \right) \\
		&= (\beta_{k0} - \beta_{j0}) + (\beta_{k1} - \beta_{j1})x_1 + \cdots + (\beta_{kp} - \beta_{jp}) x_p 
	\end{align*}
	\item In multinomial regression, the choice of the baseline predictor is unimportant.  However, the interpretation of the multinomial logistic regression model is tied to the choice of baseline.  For instance, $\beta_{k0}$ is the log odds of $Y = k$ versus $Y = K$ when all predictors are 0.
	\item \textbf{Softmax coding}, used extensively in ML, is an alternative scheme for coding multinomial logistic regression, but which is equivalent in the sense that all fitted values, log odds between any pair of classes, and other key model outputs will remain the same.  Rather than picking a baseline, all classes are symmetrically treated:
	\begin{align*}
		P(Y = k| X = x ) = \dfrac{\exp(\beta_{k0} + \beta_{k1}x_1 + \cdots + \beta_{kp}x_p )}{\sum_{i = 1}^K \exp(\beta_{i0} + \beta_{i1}x_1 + \cdots + \beta_{ip}x_p)}
	\end{align*}
	Here, we have the log odds between class $k$ and $j$ (as before, but now including $K$):
	\begin{align*}
		\log\left( \dfrac{P(Y = k| X  = x)}{P(Y = j| X = x )}\right) 
		&= \log\left(\dfrac{\exp(\beta_{k0} + \beta_{k1}x_1 + \cdots + \beta_{kp}x_p)}{\exp(\beta_{j0} + \beta_{j1}x_1 + \cdots + \beta_{jp}x_p)} \right) \\
		&= (\beta_{k0} - \beta_{j0}) + (\beta_{k1} - \beta_{j1})x_1 + \cdots + (\beta_{kp} - \beta_{jp}) x_p 
	\end{align*}
	\item {\color{brown} So why would one ever NOT use softmax?  The text doesn't go into it so I asked ChatGPT: 
	
	1. Ordinal Outcomes: Softmax coding treats each category as separate from all the others. If your outcome variable is ordinal, meaning the categories have a specific order (like "low", "medium", "high"), then it could make more sense to use an ordinal logistic regression model, which respects this order and can sometimes provide better predictions.

	2. Computational Concerns: In very large datasets or models with a high number of classes, softmax regression can be computationally expensive because it essentially requires estimating a separate model for each category. In these cases, other approaches like decision trees or random forests might be more efficient.
	
	3. Extreme Class Imbalance: In situations with extreme class imbalance (where one or a few categories have many more observations than the others), one-vs-all coding could lead to models that are biased towards the majority class. There are techniques to handle class imbalance, but in some cases, alternative strategies like one-vs-one coding could be more appropriate.
	
	4. Multilabel Problems: In problems where each observation can belong to more than one category, softmax coding may not be applicable because it assumes that each observation belongs to exactly one category. In these cases, other methods such as binary relevance (training a separate model for each category) might be more appropriate.}
\end{enumerate}
\subsection{Generative Models for Classification} 
\begin{enumerate}
	\item An alternative, and less direct approach to modeling the conditional distribution of the variable $Y$ given $X$ is to model the distribution of $X$ for each of the response classes.  Then use Bayes' theorem to flip these around to obtain estimates for the conditional distribution. 
	\item When the distribution of $X$ within each class is normal, the model is similar to logistic regression (no proof in text). 
	\item Some potential advantages over logistic regression: \begin{itemize}
		\item When there is substantial separation between two classes, parameter estimates in logistic regression can be suprisingly unstable.  The methods of this subsection do not suffer from this problem.
		\item If distribution of $X$ given $Y$ is approximately normal and the sample size is small, then the methods of this subsection may be more accurate.
	\end{itemize}
	\item Suppose $Y$ can take on one of $K\geq 2$ values.  Let $\pi_k$ denote the \textbf{prior} probability of a randomly chosen observation comes from the $k$th class.  Let $f_k(X) = P(X | Y = k)$ denote the \textbf{density function of $X$}.  \textbf{Bayes' theorem} states: 
	\begin{align*}
		P(Y = k | X = x) = \dfrac{\pi_k f_k(x)}{\sum_{i = 1}^K \pi_i f_i(x)} 
	\end{align*}
	We will use $p_k(x) = P(Y = k | X = x)$ for the \textbf{posterior}. 
	\item Estimating the prior $\pi_k$ is easy if we have a random sample of observations from the population -- we just compute the fraction of training observations that belong to the $k$th class.  
	\item Estimating the density $f_k(x)$ is much more challenging.  We will typically have to make some simplifying assumptions.  
	\item We will discuss three classifiers that use different estimates of $f_k(x)$ to estimate the Bayes classifer (the theoretically optimal classifier from training data): linear discriminant analysis, quadratic discriminant analysis, and naive Bayes.  
\end{enumerate}
\subsubsection{Linear Discriminant Analysis for $p = 1$}
\begin{enumerate}
	\item Assume $p = 1$.  We assume that $f_k(x)$ is \textbf{normal} or \textbf{Gaussian}:
	\begin{align*}
		f_k(x) = \dfrac{1}{\sqrt{2\pi} \sigma_k } \exp\left(- \dfrac{1}{2\sigma_k^2}(x - \mu_k)^2\right) 
	\end{align*}
	where $\mu_k$ and $\sigma_k^2$ are the mean and variance of $X$ for the $k$th class.  
	\item We will further assume that $\sigma_1^2 = \cdots = \sigma_K^2 = \sigma^2$.
	\item We then plug this into Bayes' theorem to obtain:
	\begin{align*}
		p_k(x) = \dfrac{\pi_k \dfrac{1}{\sqrt{2\pi} \sigma } \exp\left(- \dfrac{1}{2\sigma^2}(x - \mu_k)^2\right)}{\displaystyle \sum_{i = 1}^K \pi_i \dfrac{1}{\sqrt{2\pi} \sigma } \exp\left(- \dfrac{1}{2\sigma^2}(x - \mu_i)^2\right)}
	\end{align*}
	\item Under the above assumptions, the Bayes' classifer by computing for which value of $k$ the above posterior is largest.  
	\item We can find $\displaystyle \max_k p_k(x)$ by finding the maximum of the log of the numerator and only keeping track of terms that involve $k$.  Working out the basic calculation gives the observation that the class which maximimizes 
	\begin{align*}
		\delta_k(x) = x \cdot \dfrac{\mu_k}{\sigma^2} - \dfrac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
	\end{align*}
	will maximize $p_k(x)$. 
	\item Suppose $K = 2$ and $\pi_1 = \pi_2$. Then the Bayes classifer assigns an observation $x$ to class 1 if 
	\begin{align*}
		\delta_1(x) - \delta_2(x) &> 0 \\
		x\cdot \dfrac{\mu_1}{\sigma^2} - \dfrac{\mu_1^2}{2\sigma^2} - x \cdot \dfrac{\mu_2}{\sigma^2} + \dfrac{\mu_2^2}{2\sigma^2} &> 0 \\
		2x(\mu_1 - \mu_2) &> \mu_1^2 - \mu_2^2 
	\end{align*}
	\item The Bayes decision boundary is where $\delta_1(x) = \delta_2(x)$, which is equivalent to: 
	\begin{align*}
		x = \dfrac{\mu_1 + \mu_2}{2} 
	\end{align*}
	\item In practice, we have to estimate parameters $\mu_1, \dots, \mu_K, \pi_1, \dots, \pi_K,$ and $\sigma^2$ to implement our classifier.  \textbf{Linear discriminant analysis} (LDA) uses the following estimates:
	\begin{align*}
		\hat \mu_k &= \dfrac{1}{n_k} \sum_{i: y_i = k} x_i \\
		\hat \pi_k &= \dfrac{n_k}{n} \\ 
		\hat \sigma^2 &= \dfrac{1}{n - K} \sum_{k = 1}^K \sum_{i:y_i = k} (x_i - \hat \mu_k)^2 
	\end{align*}
	where $n$ is the total number of training observations and $n_k$ is the number of observations in the $k$th class.  
	\item Note that $\hat \sigma^2$ is a weighted average of sample variances.
	\item The term $1/(n - K)$ is a normalization factor that ensures we have an unbiased estimate of $\sigma^2$ (details?) 
	\item We then plug all of this into $\delta$ to obtain: 
	\begin{align*}
		\hat \delta_k(x) = x \cdot \dfrac{\hat \mu_k}{\hat \sigma^2} - \dfrac{\hat \mu_k^2}{2\hat \sigma^2} + \log(\hat \pi_k)
	\end{align*}
	This function is called the \textbf{discriminant function}.  The ``linear'' in LDA comes from the fact that the discriminant function $\hat \delta(x)$ is linear.  
\end{enumerate}

\vspace{.2in} 

\subsubsection{Linear Discriminant Analysis for $p > 1$}
\begin{enumerate}
	\item Now let's extend LDA classifier to the case of multiple predictors.
	\item We will assume that $X = (X_1, X_2, \dots, X_p)$ is drawn from a \textbf{multivariate Gaussian} distribution.
	\item This distribution assumes that each individual predictor follows a 1-d normal distribution, with some correlation between each pair of predictors.  
	\item If $\Var(X_1) = \Var(X_2)$ and $\Cor(X_1, X_2) = 0$, then one obtains a perfectly circular bell shape.  This bell shape is distored if the predictors are correlated or have unequal variances, in which case the bell will be elliptical.
	\item If a $p$-dimensional random variabe $X$ has a multivariate Gaussian distribution, then we write $X \sim N(\mu, \Sigma)$ where $\mu = \EE[X]$ and $\Sigma = \Cov(X)$ is the $p \times p$ covariance matrix of $X$. 
	\item The multivariate Gaussian density is defined as:
	\begin{align*}
		f(x) = \dfrac{1}{(2\pi)^{p/2} | \Sigma |^{1/2}} \exp \left( -\dfrac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \right) 
	\end{align*}
	\item The LDA classifier assumes that the observations in the $k$th class are drawn from a multivariate Gaussian distribution $N(\mu_k, \Sigma)$ where $\mu_k$ is a class-specific vector, and $\Sigma$ is a covariance matrix common to all classes.  
	\item Plugging in $\mu_k$ and $\Sigma$ into the density function, we obtain the density for the $k$th class $f_k(x)$.  This can then be plugged into the Bayes theorem.  After some computation, we learn that the Bayes classifier assigns an observation $X = x$ to the class for which:
	\begin{align*}
		\delta_k(x) = x^{T} \Sigma^{-1} \mu_k - \dfrac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k 
	\end{align*}
	Setting these equal to eachother divides the space from which $X$ is drawn into $K$ regions bounded by hyperplanes.  The planes form the Bayes decision boundary.
	\item From the data, we estimate the parameters $\mu_1, \dots, \mu_K, \pi_1, \dots, \pi_K,$ and $\sigma^2$ much as before.  LDA plugs these estimates to obtain discriminants $\hat \delta_k(x)$.  As before, we obtain LDA decision boundaries.
	\item Sometimes error rates can look low.  For instance, applying the LDA model to the {\tt Default} model, using balance and student status as predictors yields a training error rate of $2.75\%$. \begin{itemize}
		\item We should be skeptical that this really is a low rate since the test error rate -- the rate we really care about -- is typically higher than the training error rate.
		\item   Also, in the training sample, only 3.33\% defaulted.  Thus, a null classifier that classifies everyone as not defaulting will have an error rate that is only a bit higher than the LDA training error rate.
	\end{itemize} 
	\item There are two types of errors: false positives and false negatives.  A \textbf{confusion matrix} is a conventient way to display this information.  For our running problem, the confusion matrix is: 
	$$\begin{bmatrix} 
		9644 & 252 \\
		23 & 81 \end{bmatrix}$$
	\item There are a total of 333 out of the sample of 10000 who defaulted.  252 of these were missed by the LDA.  So the overall error rate, 2.75\% may look low, the error rate among those who defaulted is 252/333 = 75.7\%.
	\item \textbf{Sensitivity} is the percentage of defaulters correctly identified by the model -- 24.3\%.  The \textbf{specificity} is the percentage of non-defaulters that are correctly identified -- 9644/9667 = 99.8\%.
	\item If we would like to increase the sensitivity of our model (perhaps being able to correctly identify defaulters is a priority) we might deviate from the Bayes classifer by changing the threshold posterior probability for assigning default from 50\% to 20\%.  The new confusion matrix is: 
	$$\begin{bmatrix} 9432 & 138 \\ 235 & 195 \end{bmatrix} $$ 
	Notice that the overall error rate has increased to $(235 + 138)/10000 = 3.73\%$.  But the class specific error rate among defaulters is now 138/333 = 41.4\% which is a large improvement over the 75.7\% error rate seen in the unmodified LDA classifier.
	\item Here is a good summary of all the synonyms for things like false positives (type 1 errors) and false negatives (type 2 errors).
	
	\includegraphics[scale = 0.3]{FPFN.png}
\end{enumerate}
\subsubsection{Quadratic Discriminant Analysis} 
\begin{enumerate}
	\item \textbf{Quadratic discriminant analysis} (QDA) again assumes that the observations are drawn from a Gaussian distribution, but it assumes that each class has its own covariance matrix.  That is, for observations in the $k$th class, we have $X \sim N(\mu_k, \Sigma_k)$. 
	\item The Bayes classifer assigns an observation $X = x$ to the class for which:
	\begin{align*}
		\delta_k(x) &= - \dfrac{1}{2} x^T \Sigma_k^{-1}x + x^T \Sigma_k^{-1}\mu_k - \dfrac{1}{2} \mu_k^T\Sigma_k^{-1} \mu_k - \dfrac{1}{2} \log|\Sigma_k| + \log \pi_k 
	\end{align*}
	is largest. Here, the discriminant is quadratic in $x$. 
	\item Why would one prefer LDA to QDA or vice versa?  Note that if we have $p$ predictors, then estimating a covariance matrix requires $p(p+1)/2$ parameters.  With QDA, with a separate covariance matrix for every class, this comes to a total of $Kp(p+1)/2$ parameters.  Thus, we see LDA is less flexible than QDA and so has lower variance.  But LDA can be highly biased if the assumption of having a common covariance matrix is badly off.
	\item LDA is preferred if there are relatively few training observations and so reducing variance is crucial.  QDA is preferred if the training set is large so that the variance of the classifier is not a major concern. (refer to the section on bias-variance tradeoff for more information). 
\end{enumerate}
\subsubsection{Naive Bayes} 
\begin{enumerate}
	\item Instead of assuming that $f_k(x)$ belongs to any particular family of distributions, \textbf{naive Bayes} makes only the assumption that within the $k$th class, the $p$ predictors are independent.  This means:
	\begin{align*}
		f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p) 
	\end{align*}
	\item The reason this assumption is so powerful is that now we do not need to consider the \textbf{joint distribution} of the predictors -- only the \textbf{marginal distribution} needs to be considered.  Joint distributions are relatively easy to obtain for multivariate normal distributions -- they are summarized by the off-diagonal elements of the covariance matrix.  However, for general multivariate distributions, joint distributions can be hard to estimate.  
	\item The posterior probability in naive Bayes:
	\begin{align*}
		P(Y = k| X = x) = \dfrac{\pi_k \times f_{k1}(x_1) \times f_{k2}(x_2) \times \cdots \times f_{kp}(x_p)}{\sum_{i = 1}^K \pi_i \times f_{i1}(x_1) \times f_{i2}(x_2) \times \cdots \times f_{ip}(x_p)}
	\end{align*}
	\item To estimate the one-dimensional density functions $f_{kj}$ using training data $x_{1j}, \dots, x_{nj}$, we have options: \begin{itemize}
		\item If the predictor is quantitative, we can make the assumption that within each class, the $j$th predictor is drawn from a univariate normal distribution.  This is basically doing QDA with an additional assumption that the class-specific covariance matrix is diagonal.	
		\item If the predictor is quantitative, we can use a non-parametric estimate of $f_{kj}$.  We could do this by making a histogram which sorts the $n_k$ training observations which belong to class $k$ into bins, then model $f_{kj}$ using the histogram.  We can also use a \textbf{kernel density estimator} to get a smoothed version of the historgram. 
		\item If $X_j$ is qualitative -- simply count the proportion of training observations for the $jth$ predictor.
	\end{itemize}
	\item Naive Bayes does not necessarily outperform LDA, but is useful in situations in which $p$ is large or $n$ is small, as reducing variance is important.
\end{enumerate}

\vspace{.2in} 

\subsection{A Comparison of Classification Methods} 
\subsubsection{An Analytical Comparison} 
\begin{enumerate}
	\item We will compare analytically LDA, QDA, naive Bayes, and logistic regression on the $K$ class classification problem.  
	\item For each approach, we will assign an observation to the class that maximizes $P(Y = k| X = x)$. This is equivalent to setting a baseline class (we'll use $K$ as the baseline), and assign an observation to the class that maximizes the log odds of an observation being in class $k$ vs. class $K$.
	\begin{align*}
		\log\left(\dfrac{P(Y = k | X = x)}{P(Y = K| X = x)} \right) 
	\end{align*}
	\item For LDA, (within class densities of X normal, class specific means, shared covariance matrix), we have the following log odds:
	\begin{align*}
		\log\left(\dfrac{P(Y = k | X = x)}{P(Y = K| X = x)} \right)  = a_k + \sum_{j = 1}^p b_kj x_j 
	\end{align*}
	where 
	\begin{align*}
		a_k &= \log\left(\dfrac{\pi_k}{\pi_K} \right) - \dfrac{1}{2}(\mu_k + \mu_K)^T\Sigma^{-1}(\mu_k - \mu_K) \\
		b_k &= \text{$j$th component of } \Sigma^{-1}(\mu_k - \mu_K) 
	\end{align*}
	We see that LDA, like logistic regression, has log odds of the posterior probabilities which are linear in $x$. 
	\item For QDA (within class densities of X normal, class specific means, class specific covariance matrices), we have log odds:
	\begin{align*}
		\log\left(\dfrac{P(Y = k | X = x)}{P(Y = K| X = x)} \right)  = a_k + \sum_{j = 1}^p b_kj x_j  + \sum_{j = 1}^p \sum_{l = 1}^p c_{kjl}x_j x_l 
	\end{align*}
	where $a_k, b_{kj}, c_{kjl}$ are functions of $\pi_k, \pi_K, \mu_k, \mu_K, \Sigma_k$ and $\Sigma_K$. We see that in QDA, the log odds of the posterior probabilities are quadratic in $x$.
	\item For naive Bayes where $f_k(x) = \prod_{j = 1}^K f_{kj}(x_j)$
	\begin{align*}
		\log\left(\dfrac{P(Y = k | X = x)}{P(Y = K| X = x)} \right) = a_k + \sum_{j = 1}^p \log\left(\dfrac{f_{kj}(x_j)}{f_{Kj}(x_j)} \right) 
	\end{align*}
	This is not a linear model.  Denote by $g_{kj}(x_j) =  \log\left(\dfrac{f_{kj}(x_j)}{f_{Kj}(x_j)} \right) $. But it does take the form of a \textbf{generalized additive model} which will be further discussed later.
	\item Some observations: \begin{itemize}
		\item We know LDA is a special case of QDA where $\Sigma_i = \Sigma$ for all $i$.  In fact, we see this in the posterior log odds -- just make $c_kjl = 0$.
		\item Any classifier with a linear decision boundary is a special case of naive Bayes where $g_{kj}(x_j) = b_{kj}(x_j)$. {\color{brown} I'm trying to find some justification for this statement, and I just can't.  Neither the internet nor ChatGPT seem to strongly support this statment.}
		\item In the case where where we model $f_{kj}(x_j)$ as 1-d Gaussians, then it happens that naive Bayes looks like LDA where $\Sigma$ is diagonal.
		\item Neither QDA nor naive Bayes can be thought of as a special case of the other.  Naive Bayes is a purely additive fit, whereas QDA involves multiplication of the $x_i$s.  
	\end{itemize}
	\item Logistic regression give log odds which are linear in the predictors.  Of course, it makes no assumption about the distributions of the predictors which LDA does.  As a result, we expect LDA to perform better when the normality assumption approximately holds, and we expect logistic regression to perform better when it does not.
	\item Here are some observations about how KNN classification fits into the picture: \begin{itemize}
		\item KNN is non-parametric.  We expect this to dominate the more structured models like LDA and logistic regression when the decision boundary is highly non-linear, provided that $n$ is very large and $p$ is small.  
		\item In terms of the bias-variance tradeoff, KNN -- being so flexible -- is very unbiased and has high variance.  We want $n$ to be much larger than $p$ to effectively use KNN.
		\item If the decision boundary is non-linear, and if $n$ is modest, or $p$ is large, QDA may be preferred since its parametric form may reduce variance.
		\item Unlike logistic regression, KNN cannot tell us which predictors are important.
	\end{itemize}
\end{enumerate}

\subsubsection{An Empirical Comparison} 
\begin{enumerate}
	\item The result of empirical investigations shows that when true decision boundaries are linear, then LDA and logistic regression perform well.  When boundaries are moderately non-linear, QDA or naive Bayes may give better results.  For complicated decision boundaries, KNN can be superior.
	\item We can achieve results somewhere between LDA and QDA by including higher powers of the predictors ($X^2, X^3, etc$) in logistic regression.
\end{enumerate}

\subsection{Generalized Linear Models} 
\begin{enumerate}
	\item What if $Y$ is neither quantitative nor qualitative?  For example, if $Y$ takes on non-negative integer values, we call these values \textbf{counts} and consider them to be neither quantitative nor qualitative. 
	\item We wish to model {\tt bikers}, the number of hourly users of a bike sharing program in Washington DC.  We are going to use the following covariates: \begin{itemize}
		\item {\tt mnth} -- month of the year
		\item {\tt hr} -- hour of the day, from 0 to 23
		\item {\tt workingday} -- 1 if it is neither a weekend or holiday
		\item {\tt temp} -- normalized temperature in Celsius
		\item {\tt weathersit} -- clear, misty/cloudy, light rain/snow, heavy rain/snow
	\end{itemize}
	\item All of these predictors except {\tt temp} will be treated as qualitative variables.
\end{enumerate}
\subsubsection{Linear Regression on the Bikeshare Data} 
\begin{enumerate}
	\item The problem with applying linear regression is that we sometimes get nonsenisical predictions such as negative values for {\tt bikers}.  
	\item Another problem is heteroscedasticity -- variance is a non-constant function of the covariates.  However, linear regression assumes that the variance is constant.
	\item And of course, {\tt bikers} is integer-valued, but linear regression models $Y$ has a continuous variable.  
\end{enumerate}
\subsubsection{Poisson Regression on the Bikeshare Data} 
\begin{enumerate}
	\item The \textbf{Poisson distribution} for a random variable which takes non-negative integer values is:
	\begin{align*}
		P(Y = k) = \dfrac{e^{-\lambda} \lambda^k}{k!}, \hspace{.2in} \text{for $k = 0, 1, 2, \dots$}
	\end{align*}
	\item $\EE[Y] = \lambda$
	\item $\Var[Y] = \lambda$
	\item Let $Y$ denote the number of bike sharing program during a particular hour of the the day, under a particular set of weather conditions, and during a particular month of the year.  We could model $Y$ as a Poisson distribution with some mean $\lambda$.  
	\item In reality, we expect the mean number of users of the bike sharing program to vary as a function of the hour of the day, month of the year, weather conditions, and so forth.
	\item So we model the mean as a function of the predictors: $\lambda(X_1, \dots, X_p)$.  In particular, in Poisson regression, we model the log of $\lambda$ as linear in the predictors:
	\begin{align*}
		\log(\lambda(X_1, \dots, X_p)) = \beta_0 + \beta_1 X_1 + \dots + \beta_pX_p 
	\end{align*}
	\item To estimate the coefficients, we use maximum likelihood estimation.  Given $n$ independent observations from the Poisson regression model, the likelihood is:
	\begin{align*}
		l(\beta_0, \beta_1, \dots, \beta_p) = \prod_{i = 1}^n \dfrac{e^{-\lambda(x_i)} \lambda(x_i)^{y_i}}{y_i!}
	\end{align*}
	where $\lambda(x_i) = \exp(\beta_0 + \beta_1x_{i1} + \cdots + \beta_px_{ip})$. 
	\item Interpretation of the Poisson coefficients is that a one unit increase in $X_j$ changes $\lambda$ by a factor of $\exp(\beta_j)$.  
	\item Because in Poisson, the mean is equal to the variance, we do a better job of modeling heteroscedasticity.  In linear regression, variance is assumed to be constant.  For our Bikeshare model, we in fact have \textbf{overdispersion}, a situation in which the variance is much higher than is expected for a chosen statistical model.
\end{enumerate}
\subsubsection{Generalized Linear Models in Greater Generality}
\begin{enumerate}
	\item We have discussed three types of regression models: linear, logistic, and Poisson.  All have the following common characteristics: \begin{itemize}
		\item We assume that, conditional on the predictors, $Y$ belongs to a certain family of distributions.  For linear: normal.  For logistic: Bernoulli.  For Poisson: Poisson.
		\item Each approach models the mean of $Y$ as a function of the predictors: 
		\begin{align*}
			\text{linear:} \hspace{.2in}   \EE(Y|X_1, \dots, X_p) &= \beta_0 + \beta_1X_1 + \dots + \beta_pX_p \\
			\text{logistic:} \hspace{.2in}   \EE(Y|X_1, \dots, X_p) &= P(Y=1|X_1, \dots, X_p) \\ 
			&= \dfrac{\exp(\beta_0 + \beta_1X_1 + \dots + \beta_pX_p)}{1 + \exp(\beta_0 + \beta_1X_1 + \dots + \beta_pX_p)} \\
			\text{Poisson:} \hspace{.2in}    \EE(Y|X_1, \dots, X_p) &= \lambda(X_1, \dots, X_p) = \exp(\beta_0 + \beta_1X_1 + \dots + \beta_pX_p)
		\end{align*}
		\item Each of these examples can be expressed using a \textbf{link function} $\eta$ which when applied to the mean gives a linear function in the predictors:
		\begin{align*}
			\eta(\EE(Y|X_1, \dots, X_p)) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
		\end{align*}
		For linear: $\eta(\mu) = \mu$.  For logistic: $\eta(\mu) = \log\left(\dfrac{\mu}{1 - \mu}\right)$.  For Poisson: $\eta(\mu) = \log(\mu)$.
	\end{itemize}
	\item The Gaussian, Bernoulli, and Poisson distributions are all members of a wider class of distributions known as the \textbf{exponential family}.  We can perform a regression by modeling the response $Y$ as coming from a distribution in the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors.  Any regression approach that follows these steps is known as a \textbf{generalized linear model} (GLM).  
\end{enumerate}

\section{Resampling methods}

\begin{enumerate}
	\item \textbf{Resampling methods} involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.
	\item For example, we can repeatedly draw different samples from training data, fit a linear regression to each sample, and then examine to what extent the resulting fits differ.
	\item Resampling can be computationally expensive, but with recent advances in computing power, these costs are not prohibitive.
	\item \textbf{Cross-validation} is a resampling mthod which can be used to estimate test error associated with a learning method in order to evaluate its performance -- aka \textbf{model assessment}.
	\item \textbf{Bootstrap} measures accuracy of a parameter estimate of a given statistical learning method.
\end{enumerate}

\subsection{Cross-Validation} 
\begin{enumerate}
	\item Recall the distinction between the test error rate and the training error rate.  The test error rate is the average error rate that results when a fitted model is used on a new observation.  We judge the quality of a model by its test error rate.  We can obtain an estimate of the test error rate by using a designated test set of observations.
	\item The training error rate is easy to compute.  However, it is generally a poor estimate of the test error rate -- it can dramatically underestimate the test error rate.
	\item There are techniques to estimate the test error rate using training data.  In this section, we consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations. 
\end{enumerate}
\subsubsection{The Validation Set Approach}

\begin{enumerate}
	\item The \textbf{validation set approach} randomly divides the available set of observations into a training set and a validation set or hold-out set.  The model is fit on the training set, and the fitted model is used to predict the responses on the validation set.  The resulting validation set error rate, typically assessed using MSE in the case of a quantiative response, provides an estimate of the test error rate.
	\item We can apply the validation set approach to the problem of predicting {\tt mpg} using polynomial functions of {\tt horsepower}.  And we can do this repeatedly by divvying the available data into training and validation sets in different ways.  We obtain:
	
	\includegraphics[scale = 0.3]{validation_set.png}

	\item We can see that the validation estimate of the test error rate can be highly variable depending on the validation set.
	\item We can surmise that since statistical methods tend to perform worse with fewer observations, and since we use up a subset of the data for the validation set, the validation set error rate may tend to overestimate the test error rate.
\end{enumerate}
\subsubsection{Leave-One-Out Cross-Validation} 
\begin{enumerate}
	\item LOOCV uses the validation set approach, but uses a single observation for the validation set.  We measure the test error rate by computing the MSE of the prediction of our single observation.  We then repeat $n$ times, excluding each observation $(x_i,y_i)$, and measuring $\mathrm{MSE}_i = (y_i - \hat y_i)^2$.  
	\item THe LOOCV estimate for the test MSE is the average:
	\begin{align*}
		\mathrm{CV}_{(n)} = \dfrac{1}{n} \sum_{i = 1}^n \mathrm{MSE}_i
	\end{align*}
	\item Because we are fitting the model with $n-1$ observations -- nearly the entire data set -- we do not expect to overestimate the test error rate as in the usual validation set method.
	\item LOOCV is not variable.  The estimate produced will be the same no matter what since there is no randomness in the training/validation split. 
	\item LOOCV appears to be expensive to implement.  However, with least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as a single model fit:
	\begin{align*}
		\mathrm{CV}_{(n)} = \dfrac{1}{n} \sum_{i = 1}^n \left(\dfrac{y_i - \hat y_i}{1 - h_i}\right)^2
	\end{align*}
	where $h_i$ is the observation's leverage statistic.  Recall that an observation for which the predictor is outside the normal range of observations is referred to as having high leverage.  The leverage statistic is:
	\begin{align*}
		h_i = \dfrac{1}{n} + \dfrac{(x_i - \overline{x})^2}{\sum_{j = 1}^n (x_j - \overline{x})^2}
	\end{align*}
	{\color{brown} I need to learn more linear regression -- particularly the matrix presentation -- before I can really understand }
\end{enumerate}

\subsubsection{$k$-Fold Cross-Validation} 
\begin{enumerate}
	\item \textbf{$k$-fold CV} randomly divides the set of observations into $k$ groups, or \textbf{folds}, of approximately equal sold.  Initially, the first fold is a validation set, and the method is fit on the remaining data.  The MSE is computed.  The process is repeated $k$ times using each fold as the validation set.
	\item The $k$-fold CV estimate is computed by averaging the MSE so generated 
	\begin{align*}
		\textrm{CV}_{(k)} = \dfrac{1}{k} \sum_{i = 1}^k \textrm{MSE}_i
	\end{align*}
	\item LOOCV is a special case of $k$-fold CV where $k = n$.
	\item The obvious advantage of doing $k$-fold CV over LOOCV is computational cost. 
	\item Here is the result of $k$-fold CV on simulated data.  Smoothing splines were applied to the simulated data at various levels of flexibility:
	
	\includegraphics[scale = 0.3]{kfoldCV.png}
	\item We observe that LOOCV estimates and $10$-fold CV perform similarly.  
	\item Though the estimates may not always get the true test MSE with high accuracy, we see that the minimum MSE can be reasonably inferred from CV.  This is an important tool for model selection.
\end{enumerate}

\subsubsection{Bias-Variance Trade-Off for $k$-Fold Cross Validation} 

\begin{enumerate}
	\item Comparing $k$-fold CV to LOOCV, we should expect that since the fit of the model uses a greater proportion of the data available, LOOCV will produce less biased estimates of the test error rate.
	\item However, it turns out that LOOCV has higher variance than $k$-fold CV.  This comes from the fact that the mean of many highly correlated (highly correlated means positively correlated) quantities has higher variance than does the mean of many quantities that are not highly correlated.  One way to understand this is the formula:
	\begin{align*}
		\Var\left(\sum_{i = 1}^n X_i \right) = \sum_{i =1}^n \Var(X_i) + 2\sum_{i < j} \mathrm{Cov}(X_i, X_j) 
	\end{align*}
	If $X$ and $Y$ are positively correlated, then the covariance sum positively contributes to the variance, whereas, in the extreme where the variables are independent, the covariance terms are all 0.
	\item So we see there is a bias-variance tradeoff associated with the choice of $k$ in $k$-fold cross-validation.  When $k$ is large, we expect less bias and greater variance.  And when $k$ is small, we expect more bias and less variance.
\end{enumerate}

\subsubsection{Cross-Validation on Classification Problems} 
\begin{enumerate}
	\item Cross-validation works on classification problems in much the same was as with regression.  
	\item Rather than use MSE to quantify test error, we use the number of misclassified observations.
	\item So the LOOCV error rate is:
	\begin{align*}
		\mathrm{CV}_{(n)} = \dfrac{1}{n} \sum_{i = 1}^n \mathrm{Err}_i
	\end{align*}
\end{enumerate}

\subsection{The Bootstrap} 
\begin{enumerate}
	\item The \textbf{bootstrap} is a widely applicable and powerful statistical tool to quantify the uncertainty associated with an estimator or statistical learning method.
	\item It can be applied to a wide range of statistical learning methods, including some for which getting a measure of variability is difficult (even by statistical software).
	\item Consider the following problem: \begin{itemize}
		\item Suppose we wish to invest a fixed sum of money in two financial assets that yeild returns of $X$ and $Y$.  We will invest a fraction $\alpha$ of our money in $X$, and the remainder $1 - \alpha$ in $Y$.  We wish to choose $\alpha$ to minimize the total risk, or variance, of our investment.  So we want to minimize $\Var(\alpha X + (1 - \alpha)Y)$.
		\item One can show that the value which minimizes risk is:
		\begin{align*}
			\alpha = \dfrac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}}
		\end{align*}
		{\color{brown} This follows fairly easily from $\Var(X + Y) = \Var(X) + \Var(Y) + 2\Cov(X,Y)$, $\Var(\alpha X) = \alpha^2$, and $\Cov(\alpha X, Y) = \alpha \Cov(X,Y)$.}
		\item Using estimates for these quantities $\hat \sigma_X^2, \hat \sigma_Y^2, \hat \sigma_{XY}$ coming from data, we can obtain an estimate:
		\begin{align*}
			\hat \alpha = \dfrac{\hat \sigma_Y^2 - \hat \sigma_{XY}}{\hat \sigma_X^2 + \hat \sigma_Y^2 - 2 \hat\sigma_{XY}}
		\end{align*}
		\item Simulating 100 pairs of returns repeatedly, we can creates estimates and obtain values for $\hat \alpha$.  By doing this enough times, we can approximate the distribution of $\hat \alpha$, including the mean $\overline{\alpha}$ and the standard deviation.
		\item However, in most problems, simulations such as these are impossible since we do not know the true values of the quantities.  We also cannot repeatedly sample from the popultion.
		\item The bootstrap is to repeatedly sample with replacement from the original data set.  Repeating this $B$ times, we obtain $B$ bootstrapped samples $Z^{*1}, Z^{*2}, \dots, Z^{*B}$.
		\item Each of these bootstrapped samples can be used to create $B$ estimates of $\alpha$ $\hat\alpha^{*1}, \dots, \hat \alpha^{*B}$.
		\item We can then compute the standard error of these bootstrap estimates using the formula:
		\begin{align*}
			\mathrm{SE}_B(\hat\alpha) = \sqrt{\dfrac{1}{B- 1} \sum_{r = 1}^B \left(\hat\alpha^{*r} - \dfrac{1}{B} \sum_{s = 1}^r \hat\alpha^{*s} \right)^2}
		\end{align*}
		\item In this example, bootstrapping gives a distribution of $\hat \alpha$ very close to the ``true'' distribution coming from the simulations.
	\end{itemize}
	\item The bootstrap can estimate other quantities.  In the lab section of this chapter, there is the implmentation of the bootstrap to estimate the standard error of linear regression coefficients.  In section 3.1.2, given are formulas for the standard errors of linear regression coefficient estimates.  However, these formulas assume that errors for each observation have common variance $\sigma^2$ and are uncorrelated.  If we perform linear regression in on data generated by a non-linear system, then we should not expect these formulas to yield accurate estimates.  The bootstrap does not rely on these assumptions, and so give a better estimate of standard error.

\end{enumerate}

\section{Linear Model Selection and Regularization} 
\begin{enumerate}
	\item The standard linear model 
	\begin{align*}
		Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon 
	\end{align*}
	is typically fit using least squares.  In this chapter, we will discuss ways in which the linear can be improved with some alternative fitting procedures. 
	\item Why would we want to do this? \begin{itemize}
		\item \textbf{Prediction Accuracy:} If the true relationship is approximately linear, then least squares will have low bias.  If $n \gg p$, then the least squares will have low variance.  However, if $n$ is not so much larger, there will be lots of variance.  And if $p > n$, there is no longer a unique least squares estimate -- the variance is infinite so the method cannot be used at all.  By \textbf{constraining} or \textbf{shrinking} the estimated coefficients, we can often reduce the varaince at the cost of a small increase in bias.
		\item \textbf{Model Interpretability:} Some predictors are not associated with the response.  Keeping these irrelevant variables around leads to unnecessary complexity.  We will see approaches for performing \textbf{feature selection} or \textbf{variable selection}.
	\end{itemize}
	\item Three important classes of methods which are alternatives to least squares: \begin{itemize}
		\item \textbf{Subset selection} Identify a subset of the predictors and fit on these only.
		\item \textbf{Shrinkage} Shrink estimated coefficients toward zero.  This has the effect of reducing variance.  Some methods may shrink coefficients to zero, which has the effect of variable selection.
		\item \textbf{Dimension Reduction:} Project the $p$ predictors onto an $M$-dimensional subspace where $M < p$, and fit using $M$ projections.
	\end{itemize}
\end{enumerate}
\subsection{Subset Selection} 
\subsubsection{Best Subset Selection} 
\begin{enumerate}
	\item There are $2^p$ subsets of the $p$ predictors.  We can fit all $2^p$ models and select the one that is best.
	\item Break up the task into stages: \begin{itemize}
		\item Let $\mathcal{M}_0$ be the \textbf{null model}, which contains no predictors.  This model predicts the sample mean for each observation.
		\item For $k = 1, \dots, p$, fit all $ n \choose k $ models with $k$ predictors.  Let $\mathcal{M}_k$ be the one with lowest RSS, or, equivalently, largest $R^2$. 	
		\item Select the best model among $\mathcal{M}_0, \dots \mathcal{M}_p$ using cross-validated prediction error, $C_p$(AIC), BIC, or adjusted $R^2$. 
	\end{itemize}
	\item The reason our metric changes from RSS to something different is because naturally, the more complicated model will have a lower RSS.  However, the more complicated model will not necessarily perform well on a test set.
	\item RSS is not appropriate for all models.  In the case of logistic regression, instead of ordering by RSS, we use \textbf{deviance}. Deviance is a measure of goodness of fit for generalized linear models.  {\color{brown} It's not really explained in the text, but for a model $\mathcal{M}$, the deviance of $\mathcal{M}$ is:
	\begin{align*}
		D_{\mathcal{M}} = -2 \log\left(\dfrac{L_\mathcal{M}}{L_\mathcal{S}}\right) = -2(\log L_\mathcal{M} - \log L_\mathcal{S})
	\end{align*}
	where $\mathcal{S}$ denotes the saturated model, and $L_{\textrm{model}}$ is the likelihood of the model.  (Note that the log likelihood of the saturated model is not necessarily 0). 
	
	But we can think of the deviance of $\mathcal{M}$ as being $-2$ times the log likelihood of $\mathcal{M}$ plus a constant (which is the log likelihood of the saturated model).  This generalizes RSS for linear regression.  Try examining:
	
	\url{https://statisticaloddsandends.wordpress.com/2019/03/27/what-is-deviance/}
	
	If we are comparing two models $\mathcal{M}_0$ and $\mathcal{M}_1$ with $\mathcal{M}_0 \subset \mathcal{M}_1$ (meaning that every predictor in $\mathcal{M}_0$ is a predictor in $\mathcal{M}_1$), then the test statistic is 
	\begin{align*}
		D &= D_{\mathcal{M}_0} - D_{\mathcal{M}_1} \\
		&= -2(\log L_{\mathcal{M}_0} - \log L_{\mathcal{M}_1})
	\end{align*} 
	If $\mathcal{M}_0$ and $\mathcal{M}_1$ have $p_0$ and $p_1$ parameters, then by Wilk's theorem, under the null hypothesis that $\mathcal{M}_1$ is not a better fit, as the sample size goes to $\infty$, then $D \sim \chi^2_{p_1 - p_0}$.}

	\item Of course, best subset selection suffers from high computational cost.
	\item Also, the larger $p$ is, the higher chance of running into overfit models.
\end{enumerate}

\subsubsection{Stepwise Selection} 

\begin{enumerate}
	\item \textbf{Forward stepwise selection} begins with the null model, then adds predictors one-at-a-time, until all of the predictors are in the model.  At each stpe, the predictor that gives the greatest additional improvement is added.
	\item The number of models fit: 1 for the null model, and $p - k$ in the $k$th step.  Thus, the total number is:
	\begin{align*}
		\sum_{k = 0}^{p -1} (p - k) = 1 + \dfrac{p(p + 1)}{2} 
	\end{align*}
	\item When $p = 20$, best subset requires 1,048,576 fits.  Forward stepwise selection requires only 211.
	\item Here's the algorithm for forward stepwise selection: \begin{itemize}
		\item Find $\mathcal{M}_0$.
		\item For $k = 0, \dots p-1$, fit the $p-k$ models that augment the predictors in $\MM_k$ with one additional predictor.  Let $\MM_{k+1}$ be the model with smallest RSS.
		\item Select the best model from $\MM_0, \dots, \MM_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.
	\end{itemize}
	\item Forward stepwise selection is not guaranteed to find the best model out of the $2^p$ possible models. 
	\item This method can be used in high-dimensional settings where $n < p$.  However, since each model is fit using least squares, there will not be a unique model obtained when $p \geq n$. 
	\item \textbf{Backward stepwise selection} \begin{itemize}
		\item Fit the \textbf{full model} $\MM_p$ -- the model with all $p$ predictors.
		\item For $k = p, \dots, 1$, consider all $k$ models that contain all but one of the predictors in $\MM_k$.  Let $\MM_{k -1}$ be the model with smallest RSS.
		\item Select the best model from $\MM_0, \dots, \MM_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.	\end{itemize}
	\item Backward selection requires $n \geq p$ so that the full model can be fit.
	\item Hybrid methods combining best subset, forward stepwise and backward stepwise are also used.
\end{enumerate}

\subsubsection{Choosing the Optimal Model}
\begin{enumerate}
	\item How to determine which model among a collection of models is best?  Here best means low test error.  We know that training error can be a poor estimate of test error, so RSS and $R^2$ are not suitable metrics for determining the best model.
	\item There are two common approaches to estimate test error: \begin{itemize}
		\item Indirectly estimate test error by making an adjustment to the training error to account for bias due to overfitting.
		\item Directly estimate test error using a validation set approach or a cross-validation approach (discussed in Chapter 5).
		\end{itemize}
	\item We learned in Chapter 2 that the training MSE = RSE/n is generally an underestimate of the test MSE.  This is because when fitting a model to the training data using least squares, we seek to minimize training RSS.  We know also that the training error always descreases as more variables are added to the model, but test error could go down or up.
	\item Four approaches for adjusting the training error to obtain better estimates of test error: 
	\begin{itemize}
		\item {\textbf{$C_p$}}: for a fitted least squares model containing $d$ predictors, the $C_p$ estimate of test MSE is 
		\begin{align*}
			C_p = \dfrac{1}{n}(\mathrm{RSS} + 2d \hat \sigma^2)
		\end{align*}
		where $\hat \sigma^2$ is an estimate of the variance of the error $\epsilon$.  One can show that $C_p$ is an unbiased estimate of test MSE.  So when determining which set of models is best, we choose a model with lowest $C_p$.
		\item \textbf{AIC -- Akaike information criterion:}  This is defined for a large class of models fit by maximum likelihood.  For linear models with Gaussian errors, maximum likelihood and least squares are the same thing.  So, in this case:
		\begin{align*}
			\mathrm{AIC} = \dfrac{1}{n} (\mathrm{RSS} + 2 d \hat \sigma^2) 
		\end{align*}
		where some constants have been removed.  So, in this case, AIC and $C_p$ are proportional to one another.
		\item \textbf{BIC -- Bayesian information criterion:} is similar to AIC, except the penalty for an additional predictor is $\log(n) \hat \sigma^2$.  For least squares, up to irrelevant constants: 
		\begin{align*}
			\mathrm{BIC} = \dfrac{1}{n} (\mathrm{RSS} + \log(n)d \hat\sigma^2)
		\end{align*}
		BIC generally places a heavier penalty on models with many variables, compared to AIC.
		\item \textbf{Adjusted $R^2$:} Recall that $R^2 = 1 - \mathrm{RSS}/\mathrm{TSS}$.  It is the propotion of variance explained by the model.  However, this makes no penalty for using more variables.  For a least squares model with $d$ variables:
		\begin{align*}
			\text{Adjusted $R^2$} = 1 - \dfrac{\mathrm{RSS}/(n - d - 1)}{\mathrm{TSS}/(n - 1)}
		\end{align*}
		If a variable is added to a model which does not decrease RSS much, then the adjusted $R^2$ will tend to get smaller, whereas $R^2$ will increase.  Notice that maximizing adjusted $R^2$ is the same as minimizing $\mathrm{RSS}/(n - d - 1)$.
	\end{itemize}
	\item With the advent of abundant compute, directly estimating test error using cross-validation are hardly ever an issue, and so are attractive for model selection.
	\item When validation and cross-validation shows test error rates for models to be very similar, then we use the \textbf{one-standard-error rule}.  We calculated the stadnard error the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one stadnard error of the lowest point on the curve.  This fixes issues of variance in the estimate of test error.
\end{enumerate}

\subsection{Shrinkage Methods} 
\begin{enumerate}
	\item As an alternative to least squares, we can fit a model containing all $p$ predictors using a technique that \textbf{constrains} or \textbf{regularizes} the ceofficient estimates.  This is equivalent to \textbf{shrinking} coefficient estimates toward zero.
	\item Shrinking can significantly reduce variance.
\end{enumerate}
\subsubsection{Ridge Regression}
\begin{enumerate}
	\item Lease squares finds $\beta_0, \dots, \beta_p$ which minimizes:
	\begin{align*}
		\mathrm{RSS} = \sum_{i = 1}^n \left( y_i - \beta_0 - \sum_{j = 1}^p \beta_j X_j \right)^2 
	\end{align*}
	\item \textbf{Ridge regression} finds $\beta_0, \dots, \beta_p$ which minimizes: 
	\begin{align*}
		\mathrm{RSS} + \lambda \sum_{j = 1}^p \beta_j^2
	\end{align*}
	where $\lambda \geq 0$ is a \textbf{tuning parameter}.  The second term $\lambda \sum_{j = 1}^p \beta_j^2$ is called a \textbf{shrinkage penalty}.  
	\item The shrinking penalty will be small when the $\beta$'s are close to 0.
	\item As $\lambda \to \infty$, $\beta$'s go to 0. 
	\item Note the shrinking penalty does not apply to the intercept $\beta_0$. 
	\item It can happen that as $\lambda$ increases, a coefficient estimate first increases, then decreases.
	\item Recall that least squares coefficient estimates are \textbf{scale equivariant}: multiplying $X_j$ by a constant $c$ will scale the correpsonding least square coefficient estimate by $1/c$.  In contrast, ridge regression coefficent estimates can change substantially. To avoid this scaling issue, it is best to apply ridge regression after \textbf{standardizing the predictors} using the formula: 
	\begin{align*}
		\tilde{x}_{ij} = \dfrac{x_{ij}}{\sqrt{\frac{1}{n} \sum_{i = 1}^n (x_{ij} - \overline{x}_j)^2}}
	\end{align*}
	The final fit will not depend on the scale on which the predictors are measured.
	\item As $\lambda$ increases, the flexibility of ridge regression decreases leading to decreased variance, but increased bias.
	
	\includegraphics[scale = 0.4]{ridge_regression.png}
	\item In situations where the true data generating process is close to linear, then least squares will have low bias and high variance.  As the number of variables grows, the variance also grows.  Hence, ridge regression can perform well by exchanging a small increase in bias for a large decrease in variance.
	\item Ridge regression is computationally inexpensive.  In fact, the computations to solve the ridge regression minimzation problem can be done simultaneously for all values of $\lambda$ with computations almost identical to fitting a model using least squares.
\end{enumerate}

\subsubsection{The Lasso} 
\begin{enumerate}
	\item Ridge regression uses all $p$ predictors.  This makes model interpretation a challenge.
	\item The \textbf{lasso} overcomes this.  The lasso coefficients $\hat \beta_\lambda^L$ minimize 
	\begin{align*}
		\mathrm{RSS} + \lambda \sum_{j = 1}^p |\beta_j| 
	\end{align*}
	\item Lasso uses an $\ell_1$ norm, while ridge regression uses an $\ell_2$ norm on the coefficient vector $\beta$ (excluding $\beta_0$.)
	\item The $\ell_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when $\lambda$ is sufficiently large.  Thus, the lasso performs variable selection.
	
	\includegraphics[scale = 0.4]{lasso.png}
	\item One can show that the lasso and ridge regression solve the following minimization problems: 
	\begin{align*}
		\mathrm{min}_\beta \left\{ \sum_{i = 1}^n \left( y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 \right\} \hspace{.2in} &\text{subject to} \hspace{.2in} \sum_{j = 1}^p |\beta_j| \leq s \\
		\mathrm{min}_\beta \left\{ \sum_{i = 1}^n \left( y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 \right\} \hspace{.2in} &\text{subject to} \hspace{.2in} \sum_{j = 1}^p \beta_j^2 \leq s 
	\end{align*}
	That is, there exists an $s$ such that solving the above gives the lasso or ridge regression coefficients for some given value of $\lambda$.  {\color{brown} These are convex optimization problems.  Convex optimization is an huge enterprise, but I imagine the mathematical details here are not so bad.
	\item Let's try to understand how we get from the constrained problem to the unconstrained problem.  
	
	Recall the idea behind optimization via Lagrange multipliers.  Our goal is to find a point $x^*$ at which $f(x)$ is minimized, subject to a constraint $g(x) = 0$. Such a point must have the property that the gradient of $f$ is orthogonal to the codimension-1 level set $g(x) = 0$, otherwise it would be possible to nudge the point a little bit along the constraint to obtain a smaller value.  Since $\grad g$ is orthogonal to the constraint level set, we must have that $\grad f$ is a scalar multiple of $\grad g$.  
	We build the Lagrangian function: 
	\begin{align*}
		L(x, \lambda) = f(x) + \lambda g(x)
	\end{align*}
	To find possible extrema, we set $\grad L$ equal to the zero vector and solve.  

	Now suppose that the constraint is an inequuality $g(x) \leq 0$.  This defines a constraint region in which $x^*$ must lie.  Then either $x^*$ lies on the interior $g(x) < 0$ (the constraint is said to be inactive) or on the boundary $g(x) = 0$ (the constraint is said to be active).  The reason for this terminology is that if $x^*$ is in the interior, then $g(x)$ plays no role in minimizing $f(x)$ -- we can treat the problem as an unconstrained problem on $f(x)$.
	
	If $x^*$ lies on the boundary $g(x) = 0$, then notice that $\grad g$ must point out toward the exterior.  However, $\grad f$ must point into the interior -- otherwise we could get a smaller value for $f$ by moving $x$ into the interior.  Therefore, we require
	\begin{align*}
		\grad f = -\lambda \grad g
	\end{align*}
	where $\lambda > 0$.  

	Notice that we can capture both of these cases ($x^*$ in the interior or on the boundary) with the condition $\lambda g(x)  = 0$.  This implies that either $\lambda = 0$ (in which case we are performing an unconstrained optimization of $f(x)$) or $g(x)$ = 0 (in which case, we are doing a standard Lagrange optimization.)

	So, we can reformulate our minimization with inequality constraint problem as optimizing the Lagrange function 
	\begin{align*}
		L(x, \lambda) = f(x) + \lambda g(x)
	\end{align*} 
	subject to the conditions:
	\begin{align*}
		g(x) \leq 0 \\
		\lambda \geq 0 \\
		\lambda g(x) = 0
	\end{align*}
	These are known as Karush-Kuhn-Tucker (KKT) conditions.

	From here, it is straightforward to see how we get from the constrained version of ridge and lasso, to the unconstrained version.  

	Most of this material came from Appendix E of \emph{Pattern Recognition and Machine Learning} by CM Bishop.


	}
	\item Best subset selection can be fit into this optimization with constraint paradigm: 
	\begin{align*}
		\mathrm{min}_\beta \left\{ \sum_{i = 1}^n \left( y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 \right\} \hspace{.2in} &\text{subject to} \hspace{.2in} \sum_{j = 1}^p I(\beta_j \neq 0) \leq s 
	\end{align*}
	\item Increasing $\lambda$ causes the coefficients in each case to get smaller.  Increasing $s$ causes the coefficients in each case to get larger.
	\item Note that the constraints in each case correspond to $\beta$ lying in a polytope in the lasso case, and in a spherical ball in the ridge case.  Note that the vertices of the lasso polytope lie precisely on the axes where one of the $\beta$'s is equal to 0.
	\item The contours of the RSS term common to both problems look like ellipses with $\hat \beta$, the least squares estimate at the center.  If $\hat \beta$ lies in the constraint region, then the lasso or ridge regression coefficient estimates are the same as the least squares estimate.
	\item The reason that lasso tends to result in coefficient estimates exactly equal to 0 is that the level curves of RSS radiate away from $\hat \beta$ in ellipses, with larger ellipses corresponding to larger values of RSS.  The optimum value for $\beta$ will occur at the smallest ellipse which touches the constraint region.  In lasso, the constraint region has corners at the axes where one of the $\beta$s is 0.  It is likely tht the level curves of RSS will first touch at the corners thus giving rise to the variable selection property of the lasso.

	\includegraphics[scale = 0.4]{lasso_RSS_minimize.png}

	\item Clearly, lasso is superior to ridge w.r.t. interpretability.
	\item In general, one expects lasso to perform better when a small number of predictors have substantial coefficients with the rest of the coefficients small or zero.  Ridge will perform better with many predictors all of whose coefficients are roughly the same size.  
	\item Here is a simple special case of ridge and lasso.  Let's consider $n = p$, and the design matrix $X = [x_{ij}]$ to be the identity matrix.  To further simplify, assume that we are performing regression without an intercept.  \begin{itemize}
		\item The usual least squares problem in this case is  to minimize:
		\begin{align*}
			\sum_{j = 1}^p (y_j - \beta_j)^2
		\end{align*}
		The solution here is clearly $\hat \beta_j = y_j$.
		\item Ridge regression minimizes:
		\begin{align*}
			\sum_{j = 1}^p (y_j - \beta_j)^2 + \lambda\sum_{j = 1}^p \beta_j^2
		\end{align*}
		{\color{brown} Denote the above expression $f(\beta)$.
		\begin{align*}
			\dfrac{\partial f}{\partial \beta_j} &= -2(y_j - \beta_j) + 2\lambda \beta_j \\
			\dfrac{\partial^2 f}{\partial \beta_j^2} &= 2 + 2\lambda > 0\\
			\dfrac{\partial^2 f}{\partial \beta_j \partial \beta_k} &= 0, \hspace{.2in} j \neq k 
		\end{align*} 
		Set the first derivative equal to 0 and solve for $\beta_j$: 
		\begin{align*}
			0 &= -2(y_j - \beta_j) + 2\lambda \beta_j \\ 
			0 &= -y_j + \beta_j(1 + \lambda) \\ 
		\end{align*} } 
		Therefore, the ridge regression coefficients are $$\beta^R_j = \dfrac{y_j}{1 + \lambda}$$
		
		\item Lasso regression minimizes: 
		\begin{align*}
			\sum_{j = 1}^p (y_j - \beta_j)^2 + \lambda\sum_{j = 1}^p |\beta_j|
		\end{align*}
		{\color{brown} Denote the above expression $f(\beta)$: 
		\begin{align*}
			\dfrac{\partial f}{\partial \beta_j} &= -2(y_j - \beta_j) + \lambda \mathrm{sgn(\beta_j)} \\
			\dfrac{\partial^2 f}{\partial \beta_j^2} &= 2 > 0\\
			\dfrac{\partial^2 f}{\partial \beta_j \partial \beta_k} &= 0, \hspace{.2in} j \neq k 
		\end{align*}
		Set the first derivative equal to 0 and solve for $\beta_j$.  We do this in cases:
		
		If $\beta_j > 0$, then $\beta_j = y_j - \dfrac{\lambda}{2}$. Notice that if $y_j < \dfrac{\lambda}{2}$, then the right hand side is negative.  But since we assumed $\beta_j > 0$, we see that $y_j < \dfrac{\lambda}{2}$ implies the equation has no solutions.

		If $\beta_j < 0$, then $\beta_j = y_j + \dfrac{\lambda}{2}$. Notice that if $y_j > -\dfrac{\lambda}{2}$, then the right hand side is positive.  But since we assumed $\beta_j < 0$, we see that $y_j < -\dfrac{\lambda}{2}$ implies the equation has no solutions. 

		In the cases where there are no solutions, the only critical point occurs at $\beta = \overrightarrow{0}$, where the derivative is undefined.}
		
		The lasso estimates are: 
		\begin{align*}
			\hat \beta_j = \begin{cases} 
				y_j - \lambda/2 &\text{if } y_j > \lambda/2 \\
				y_j + \lambda/2 &\text{if } y_j < -\lambda/2 \\
				0 & \text{if } |y_j| \leq \lambda/2 
			\end{cases} 
		\end{align*}
		We see lasso estimtes exhibit \textbf{soft-thresholding}, in which least squares coefficients which are sufficiently small get shrunk all the way to 0.
		\item For more complicated design matrices $X$, the story is more complicated.  However, ridge regression more or less shrinks least squares estimates by the same proportion, and lasso shrinks all coefficients toward zero by a constant amount, with soft-thresholding taking sufficiently small coefficients to 0.
	\end{itemize}
	\item We can give a Bayesian interpretation for both ridge and lasso.  Suppose the coefficient vector $\beta$ has a prior $p(\beta)$.  Denote the likelihood $f(Y|X, \beta)$. Then the posterior distribution is proportional to the product of the likelihood and the prior: 
	\begin{align*}
		p(\beta | X, Y) \propto f(Y|X, \beta) p(\beta)
	\end{align*}
	Assume the linear model 
	\begin{align*}
		Y = \beta_0  + X_1\beta_1 + \dots + X_p \beta_p + \epsilon
	\end{align*}
	supposing the errors are independent and drawn from a normal distribution.  
	Further, assume that 
	\begin{align*}
		p(\beta) = \prod_{j = 1}^p g(\beta_j) 
	\end{align*}
	for some density function $g$.   Then \begin{itemize}
		\item If $g$ is a Gaussian with mean zero and standard deviation a function of $\lambda$, then the \textbf{posterior mode} for $\beta$ -- the most likely value of $\beta$ given the data -- is given by the ridge regression solution.  
		\item If $g$ is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of $\lambda$, then it follows that the posterior mode for $\beta$ is the lass solution.  
	\end{itemize}
	\item Becuase the Laplace distribution is steeply peaked at 0, the lasso expects that, a priori, many of the coefficients are exactly 0.
	\item {\color{brown} According to ChatGPT, ordinary least squares can be fit into this Bayesian paradigm with a uniform or flat prior.}
\end{enumerate}

\subsubsection{Selecting the Tuning Parameter} 

\begin{enumerate}
	\item To select a value for the tuning parameter $\lambda$, we choose a grid of values for $\lambda$, and compute the cross-validation error for each value of $\lambda$.  We choose the tuning parameter for which the cross-validation error is smallest.  Finally, the model is re-fit using all data and the chosen $\lambda$.
\end{enumerate}

\subsection{Dimension Reduction Methods} 

\begin{enumerate}
	\item \textbf{Dimension reduction methods} attempt to reduce variance by fitting a least squares model using transformed predictors.
	\item Let $Z_1, ... , Z_M$ be $M < p$ linear combinations of predictors: 
	\begin{align*}
		Z_m = \sum_{j = 1}^p \phi_{jm}X_j 
	\end{align*}
	We then fit the linear regression model:
	\begin{align*}
		y_i = \th_0 + \sum_{m = 1}^M \th_m z_{im} + \epsilon_i, \hspace{.2in} i = 1, \dots, n
	\end{align*}
	using least squares.  
	\item Notice that 
	\begin{align*}
		\sum_{m = 1}^M \th_m z_{im} = \sum_{m = 1}^M \th_m \sum_{j = 1}^p \phi_{jm}x_{ij} = \sum_{j = 1}^p \sum_{m = 1}^M \th_m \phi_{jm}x_{ij} = \sum_{j = 1}^p \beta_j x_{ij} 
	\end{align*}
	where 
	\begin{align*}
		\beta_j = \sum_{m = 1}^M \th_m \phi_{jm}
	\end{align*}
	So dimension reduction is a special case of fitting the linear model, but now with additional constraints on the regression coefficients.
	\item  Note that if $M = p$ and the $Z_m$ are linearly independent, then the constraints imposed on $\beta_j$ are vacuous, and so no dimension reduction occurs.  Fitting using the new variables $Z_1, \dots, Z_p$ is equivalent to performing least squares on the original predictors. 
\end{enumerate}

\subsubsection{Principle Components Regression} 
\begin{enumerate}
	\item \textbf{Principal Components Analysis} (PCA) is a technique for reducing the dimension of an $n \times p$ data matrix $X$.  
	\item The \textbf{first principal component} direction of the data is that along which the observations vary the most.  If we project the data onto a line with this direction, then we will see the most variability/spread. {\color{brown} The first principal component is the eigenvector of the sample covariance matrix $X^tX$ with largest eigenvalue (note that the sample covariance matrix is symmetric and positive semidefinite so it has non-negative eigenvalues.
	
	To find the next principal component, the previous principal component is subtracted from $X$.  I'll write down all the details at some point.  In particular, there is a presentation of the PCA using the singular value decomposition of $X$.  This avoids having to compute $X^tX$ which can be very expensive.)}
	\item PCR works well when the first few principal components suffice to capture most of the variation in the predictors as well as the relationship with the response.  This is because fitting with $M << p$ variables will mitigate overfitting.
	\item Note that PCR is not a feature selection model since it uses linear combinations of the original features.
	\item Cross-validation is typically used to select the number of principal components $M$ used in PCR.
	\item Standardizing predictors is generally recommended prior to using PCR.  If the variables all have the same units, this may be skipped.
	\item Note that PCR is unsupervised -- the value of $Y$ has no bearing on the principal components.  Consequently, there is no guarantee that the directions that best explain the predictors will also be the best for predicting the response.
\end{enumerate}

\subsubsection{Partial Least Squares} 
\begin{enumerate}
	\item \textbf{Partial Least Squares} (PLS) is a supervised alternative to PCR.  It uses $Y$ to identify new features $Z_1, \dots, Z_M$ that not only approximate old features well, but are also related to the response.
	\item After standardizing the $p$ predictors, PLS computes the first direction $Z_1$ by setting each $\phi_{j1}$ in the equation: 
	\begin{align*}
		Z_1 = \sum_{j = 1}^p \phi_{j1} X_j
	\end{align*}
	equal to the coefficient of simple linear regression of $Y$ onto $X_j$.  This coefficient is proportional to $\Cor(Y, X_j)$. 
	{\color{brown} Recall from 3.1.1., the formula: 
	\begin{align*}
		\hat\beta_1 &= \dfrac{ \sum_{i = 1}^n (x_i - \overline{x} )(y_i - \overline{y})}{\sum_{i = 1}^n (x_i - \overline{x} )^2} \\
	\end{align*}
	Notice that since we have standardized the predictors, that the denominator is $n-1$.}
	Therefore, in computing $Z_1$, PLS places the highest weight on variables that are most strongly related to the response.
	\item To obtain $Z_2$, we adjust each of the variables for $Z_1$ by regressing each variable on $Z_1$ and taking residuals.  These residuals can be interpreted as the remaining information that has not been explained by the first PLS direction.  {\color{brown}  I rather enjoy the conflation here of ``information'' and ``variation'' -- at least, that's what I suspect is going on.}  We then compute $Z_2$ using this \textbf{orthogonalized} data in the same way that $Z_1$ was computed.  This term is used since the vector of residuals is orthogonal to the original data: $(Y - \hat Y)^t \hat Y= 0 $.
	\item The number of partial least squares directions $M$ is chosen by cross-validation.
	\item In practice, PLS performs no better than ridge regression or PCR.  
	\item Supervised dimension reduction can reduce bias, but it also can increase variance making the overall benefit of PLS relative to PCR a wash.
\end{enumerate}

\subsection{Considerations in High Dimensions}

\subsubsection{High-Dimensional Data} 
\begin{enumerate}
	\item Most traditional techniques for regression and classification are intended for \textbf{low-dimensional} data sets in which $n \gg p$.
	\item The \textbf{high-dimensional setting} is when $p \gg n$.  
	\item Some examples of problems: If you have a set of 200 patients you wish to study for some sort of condition, you may include in your set of predictors the nearly 500,000 single nucleotide polymorphisms (SNPs) -- individual DNA mutations that are relatively common in the popultion.
	\item Connecting online shopping patterns with search terms is another example where $p \gg n$.  
\end{enumerate}

\subsubsection{What Goes Wrong in High Dimensions?}
\begin{enumerate}
	\item As $p$ gets larger relative to a fixed $n$, we see the model becoming more and more flexible.  This allows for better and better fits at the training stage, but worse performance at validation because of overfitting.
	\item Notice that if $p = n$, then a tried-and-true technique like least squares will give a perfect fit to the data since there are as many variables as there are constraints.  
\end{enumerate}

\subsubsection{Regression in High Dimensions} 
\begin{enumerate}
	\item Less flexible least squares models like forward stepwise selection, ridge regression, the lasso and PCR are useful for high dimensions since we avoid overfitting.
	
	\includegraphics[scale = 0.3]{high_dim.png}

	\item Notice a few things from the above figure: \begin{itemize}
		\item regularization/shrinkage is important in high-dimensional settings.
		\item tuning parameter selection is crucial.
		\item test error tends to increase as the dimensionality increases unless the additional features are truly associated with the response.
	\end{itemize}

	\item On the third point above, note that adding signal features does improve the fitted model.  But adding noise features will not since the risk of overfitting goes up as the dimensionality increases.
	\item Therefore, technologies that allow for massive collection of measurements for thousands or millions of features are double-edged -- they will lead to improved predictive models if these features are in fact relevant to the problem at hand, and will lead to worse results if not. And, even if they are relevant, the variance incurred by adding the variable may outweigh the reduction in bias they bring.
\end{enumerate}

\subsubsection{Interpreting Results in High Dimensions}
\begin{enumerate}
	\item Multicollinearity, correlation among predictors, is an extreme problem in high dimensions.  Any variable can be written as a linear combination of all the other variables in the model, and so we can never know exactly which variables (if any) are truly predictive of the outcome.
	\item For example, if one were trying to predict high blood pressure using SNPs, and forward stepwise selection indicates 17 SNPs which lead to a good predictive model on the training data, it would be incorrect to conclude that these SNPs predict bp more effectively than some other set of 17 SNPs.  It is important for such a model to be further validated on independent data sets.
	\item Reporting errors and measures of fit cannot be done in the high-dimensional setting with squared errors, $p$-values, $R^2$, etc.  This is because we know that $p > n$ can give useless models with zero residuals.  
	\item However, the values of these quantities on an independent test set is valid.
\end{enumerate}
\end{document}


